---
title: "modeling_pca"
author: "Sardhendu Mishra"
date: "4/20/2018"
output: html_document
---

```{r}
outpath = '/Users/sam/All-Program/App-DataSet/DataScienceProjects/CreditCardFraud/'
dat = read.csv(paste(outpath,"sample_credit_fraud.csv"))
dat.scaled = cbind(scale(dat[,2:30]), dat[,31])
colnames(dat.scaled)[30] <- c("Class")
colMeans(dat.scaled)
head(dat.scaled, 2)
```

```{r}
dat.scaled = scale(dat[,2:29])
head(dat.scaled)

# Perform PCA
dat.pca = prcomp(dat.scaled)
names(dat.pca)

# Just for sanity check see if the columns are 0 mean
# dat.pca$center
# dat.pca$scale

# Check the C
dat.pca$rotation[1:5, 1:5]
biplot(dat.pca, scale = 0)


# Get the proportion of variability explained by each variable
std_dev = dat.pca$sdev
pr_var = std_dev^2
pr_varexp = pr_var/sum(pr_var)

# Plot the explained variability
plot(pr_varexp, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")

plot(cumsum(pr_varexp), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")

dat.reduced = dat.pca$x[,1:25]
head(dat.reduced)
```


```{r}
library('rjags')
model_string = "model{
  for (i in 1:length(y)){
    y[i] ~ dbern(p[i])
    logit(p[i]) = b0 + b[1] * PC1[i] + b[2] * PC2[i] + b[3] * PC3[i] + b[4] * PC4[i] + b[5] * PC5[i] + b[6] * PC6[i] + b[7] * PC7[i] + b[8] * PC8[i] + b[9] * PC9[i] + b[10] * PC10[i] + b[11] * PC11[i] + b[12] * PC12[i] + b[13] * PC13[i] + b[14] * PC14[i] + b[15] * PC15[i] + b[16] * PC16[i] + b[17] * PC17[i] + b[18] * PC18[i] + b[19] * PC19[i] + b[20] *PC20[i] + b[21] *PC21[i] + b[22] *PC22[i] + b[23] *PC23[i] + b[24] *PC24[i] + b[25]*PC25[i]
        }
  }
  
  # We chouse priors for the parameters
  b0 ~ dnorm(1.0, 1/25)               # Equivallent to a non-informative prior
  for (j in 1:25){  
    b[j] ~ dnorm(0.0, 1/25.0) 
  }
}"
```

```{r}
set.seed(632)

dim(dat[,'Amount'])
# Jags takes input as list
data_jags = list(y=dat.scaled$Class, PC1=dat.reduced[,'PC1'], PC2=dat.reduced[,'PC2'], PC3=dat.reduced[,'PC3'], PC4=dat.reduced[,'PC4'], PC5=dat.reduced[,'PC5'], PC6=dat.reduced[,'PC6'], PC7=dat.reduced[,'PC7'], PC8=dat.reduced[,'PC8'], PC9=dat.reduced[,'PC9'], PC10=dat.reduced[,'PC10'], PC11=dat.reduced[,'PC11'], PC12=dat.reduced[,'PC12'], PC13=dat.reduced[,'PC13'], PC14=dat.reduced[,'PC14'], PC15=dat.reduced[,'PC15'], PC16=dat.reduced[,'PC16'], PC17=dat.reduced[,'PC17'], PC18=dat.reduced[,'PC18'], PC19=dat.reduced[,'PC19'], PC20=dat.reduced[,'PC20'], PC21=dat.reduced[,'PC21'], PC22=dat.reduced[,'PC22'], PC23=dat.reduced[,'PC23'], PC24=dat.reduced[,'PC24'], PC25=dat.reduced[,'PC25'])


# Parameters that we would wanna monitor
params = c('b0','b')

mod3 = jags.model(textConnection(model_string), data=data_jags, n.chains = 3)

# Burn-in period
update(mod3, 1e3)

mod3_sim = coda.samples(model=mod3, variable.names = params, n.iter = 10e3)

mod3_csim = as.mcmc(do.call(rbind, mod3_sim))
```

```{r, fig.width=10, fig.height=10}
pdf("modeled_mcmc_dexpmodel_trace.pdf", width=20, height=20)
plot(mod3_sim)
dev.off
```

```{r}
gelman.diag(mod3_sim)
```
```{r}
autocorr.diag(mod1_sim)
pdf("modeled_mcmc_dexpmodel_acorr.pdf",width=20,height=15)
autocorr.plot(mod1_sim)
dev.off
```
```{r}
effectiveSize(mod3_sim)
```

```{r}
dic1 = dic.samples(mod3, n.iter=1e3)
dic1
```

# PREDICTIONS:
```{r}
pm_coef = colMeans(mod3_csim) # Get the means of the b coefficient. Also equivallent to getting the learned parameters
pm_Xb = pm_coef ['b0'] + dat.scaled[, 1:29] %*% pm_coef[1:29]

phat = 1.0 / (1.0 + exp(-pm_Xb))
head(phat)
```

# ANALYSIS
```{r}
library(ggplot2)

performanceMetric <- function (cutoffRange, y, y_hat){
    y_bin <- y_hat
    actualYesIndex <- which(y==1)
#     perfMetric <- data.frame()
    perfMetric <- matrix(0,length(cutoffRange),3)    # 3 is because we calculate accuracy, recall and precision
    for (i in 1:length(cutoffRange)){
#         print (cutOFF)
        predYesIndex <- which(y_hat>=cutoffRange[i])
        bothYesIndex <- intersect(actualYesIndex,predYesIndex)

        # Get the Binomial prediction based on cut-off value
        y_bin[predYesIndex] <- 1
        y_bin[-predYesIndex] <- 0

        # Calculate the accuracy, precision and recall
        accuracy <- length(which(y_bin == y))/length(y)
        precision <- length(bothYesIndex)/length(predYesIndex)
        recall <- length(bothYesIndex)/length(actualYesIndex)
        cbind(accuracy, precision, recall)
        
        perfMetric[i,] <- cbind(accuracy, precision, recall)
    }
    
    return (perfMetric)
 
}

plotPerfMetric <- function(performanceDF, cutoffRange){
    p <- ggplot() + 
      geom_line(data = performanceDF, aes(x = cutoffRange, y = accuracy, color = "accuracy")) +
      geom_line(data = performanceDF, aes(x = cutoffRange, y = precision, color = "precision")) +
      geom_line(data = performanceDF, aes(x = cutoffRange, y = recall, color = "recall")) +
      xlab('Cutoff') +
      ylab('percent.change')
    return (p)
}

# cbind(phat, dat$Class)

cutoffRange <- seq(.01,.99,length=1000)
perfMatrix <- performanceMetric(cutoffRange = cutoffRange, 
                                y = dat$Class, 
                                y_hat = phat)

perfDF <- data.frame(perfMatrix)
names(perfDF) <- c('accuracy', 'precision', 'recall')
head(perfDF)

# Plot Accuracy, precision and recall
options(repr.plot.width=6, repr.plot.height=4)
p <- plotPerfMetric(perfDF, cutoffRange)
p
```