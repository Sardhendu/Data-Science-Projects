head(y1)
head(y2)
# We compute probabilities and plot the number of doctor visits against its probility. As you can see the plots make complete sense.
#### NOTE the levels below shows the number of doctor visits.
plot(table(factor(y1, levels=0:20))/length(y1), xlab = 'Number of doctor visits (good health)', ylab = 'Probability of doctor visits (good health)')
points(table((y2+0.1))/length(y2), xlab = 'Number of doctor visits (bad health)', ylab = 'Probability of doctor visits (bad health)', col='red')
# Finally answer to outquestion:
# What is the probability of doctor visits of person 1 will be more than person 2
mean(y2>y1)
var(c(4,5,6))
library("car")
data(Leinhardt)
dat = na.omit(Leinhardt)
# Log Transform.
dat$logincome = log(dat$income)
dat$loginfant = log(dat$infant)
str(dat)
head(dat)
mod_string = "model{
# Note the intercept term a is separate for each region, while b[1] and b[2] are constant
for  (i in 1: length(y)){
y[i] ~ dnorm(mu[i], prec)
mu[i] = a[region[i]] + b[1]*log_income[i] + b[2]*is_oil[i]
}
# Here we model the parameter a for each region
for (j in 1:length(region)){
a[i] ~ dnorm(a0, prec_a)
}
# Now we model the hyperparameter for the intercept parameter a. We assume all a's come from the same distribution. This
# assumption is what makes hierarchial distribution different from modeling seperate models for each model with separate
# parameter. Our assumprtion of all a's comes from the same distribution is what makes hierarchial model share information
# between different group which is the case in the real world.
prior_sample_size = 1
prior_guess_for_variance = 10
a0 ~ dnorm(0.0, 1.0/1.0e6)            # Flat normal distribution [As good as non-informative prior]
prec_a ~ dgamma(prior_sample_size/2.0, prior_sample_size*prior_guess_for_variance/2.0)
# Set the prior for likelihood:
# Here we model the parameters b[1] and b[2], note these are constant
for (k in 1:2){
b[k] ~ dnorm(0.0, 1.0/1.0e6)
}
prior_sample_size_1 = 5
prior_guess_for_variance_1 = 10
prec ~ dgamma(prior_sample_size/2.0, prior_sample_size*prior_guess_for_variance/2.0)
# The output variable, to check variability
sig = sqrt(1.0/prec)
}"
mod_string = "model{
# Note the intercept term a is separate for each region, while b[1] and b[2] are constant
for  (i in 1: length(y)){
y[i] ~ dnorm(mu[i], prec)
mu[i] = a[region[i]] + b[1]*log_income[i] + b[2]*is_oil[i]
}
# Here we model the parameter a for each region
for (j in 1:length(region)){
a[i] ~ dnorm(a0, prec_a)
}
# Now we model the hyperparameter for the intercept parameter a. We assume all a's come from the same distribution. This
# assumption is what makes hierarchial distribution different from modeling seperate models for each model with separate
# parameter. Our assumprtion of all a's comes from the same distribution is what makes hierarchial model share information
# between different group which is the case in the real world.
prior_sample_size = 1
prior_guess_for_variance = 10
a0 ~ dnorm(0.0, 1.0/1.0e6)            # Flat normal distribution [As good as non-informative prior]
prec_a ~ dgamma(prior_sample_size/2.0, prior_sample_size*prior_guess_for_variance/2.0)
tau = sqrt(1.0/prec_a)
# Set the prior for likelihood:
# Here we model the parameters b[1] and b[2], note these are constant
for (k in 1:2){
b[k] ~ dnorm(0.0, 1.0/1.0e6)
}
prior_sample_size_1 = 5
prior_guess_for_variance_1 = 10
prec ~ dgamma(prior_sample_size/2.0, prior_sample_size*prior_guess_for_variance/2.0)
# The output variable, to check variability
sig = sqrt(1.0/prec)
}"
data_jags = list(y=dat$loginfant, log_income=dat$logincome, is.oil=as.numeric(dat$oil=="yes"), region=as.numeric(dat$region))
set.seed(116)
head(data_jags)
set.seed(116)
data_jags = list(y=dat$loginfant, log_income=dat$logincome, is_oil=as.numeric(dat$oil=="yes"), region=as.numeric(dat$region))
head(data_jags)
table(data_jags$is_oil, data_jags=="yes")
table(data_jags$is_oil, data_jags$region)
set.seed(116)
data_jags = list(y=dat$loginfant, log_income=dat$logincome, is_oil=as.numeric(dat$oil=="yes"), region=as.numeric(dat$region))
head(data_jags)
# Sanity Check: Since the oil variable is also a count variable, it is good to check if the oil exporting countries are all in the same region. If that is the case then we would never know if it was oil of region intercept that affected the infant (response variable). Howvere, thats not the case, so no problem, we just go ahead. It seems oil_expoting countries ar in region 1,2,3.
table(data_jags$is_oil, data_jags$region)
params = c("a0", "a", "b", "sig", "tau")
set.seed(116)
data_jags = list(y=dat$loginfant, log_income=dat$logincome, is_oil=as.numeric(dat$oil=="yes"), region=as.numeric(dat$region))
head(data_jags)
# Sanity Check: Since the oil variable is also a count variable, it is good to check if the oil exporting countries are all in the same region. If that is the case then we would never know if it was oil of region intercept that affected the variability in infant mortality (response variable). Howvere, thats not the case, so no problem, we just go ahead. It seems oil_expoting countries ar in region 1,2,3.
table(data_jags$is_oil, data_jags$region)
params = c("a0", "a", "b", "sig", "tau")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
mod_sim = coda.samples(model=mod, variable.names = params, n.iter = 5e3)
mod_string = "model{
# Note the intercept term a is separate for each region, while b[1] and b[2] are constant
for  (i in 1: length(y)){
y[i] ~ dnorm(mu[i], prec)
mu[i] = a[region[i]] + b[1]*log_income[i] + b[2]*is_oil[i]
}
# Here we model the parameter a for each region
for (j in 1:length(region)){
a[i] ~ dnorm(a0, prec_a)
}
# Now we model the hyperparameter for the intercept parameter a. We assume all a's come from the same distribution. This
# assumption is what makes hierarchial distribution different from modeling seperate models for each model with separate
# parameter. Our assumprtion of all a's comes from the same distribution is what makes hierarchial model share information
# between different group which is the case in the real world.
prior_sample_size = 1
prior_guess_for_variance = 10
a0 ~ dnorm(0.0, 1.0/1.0e6)            # Flat normal distribution [As good as non-informative prior]
prec_a ~ dgamma(prior_sample_size/2.0, prior_sample_size*prior_guess_for_variance/2.0)
tau = sqrt(1.0/prec_a)
# Set the prior for likelihood:
# Here we model the parameters b[1] and b[2], note these are constant
for (k in 1:2){
b[k] ~ dnorm(0.0, 1.0/1.0e6)
}
prior_sample_size_1 = 5
prior_guess_for_variance_1 = 10
prec ~ dgamma(prior_sample_size/2.0, prior_sample_size*prior_guess_for_variance/2.0)
# The output variable, to check variability
sig = sqrt(1.0/prec)
}"
mod_sim = coda.samples(model=mod, variable.names = params, n.iter = 5e3)
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
mod_string = "model{
# Note the intercept term a is separate for each region, while b[1] and b[2] are constant
for  (i in 1: length(y)){
y[i] ~ dnorm(mu[i], prec)
mu[i] = a[region[i]] + b[1]*log_income[i] + b[2]*is_oil[i]
}
# Here we model the parameter a for each region
for (j in 1:length(region)){
a[j] ~ dnorm(a0, prec_a)
}
# Now we model the hyperparameter for the intercept parameter a. We assume all a's come from the same distribution. This
# assumption is what makes hierarchial distribution different from modeling seperate models for each model with separate
# parameter. Our assumprtion of all a's comes from the same distribution is what makes hierarchial model share information
# between different group which is the case in the real world.
prior_sample_size = 1
prior_guess_for_variance = 10
a0 ~ dnorm(0.0, 1.0/1.0e6)            # Flat normal distribution [As good as non-informative prior]
prec_a ~ dgamma(prior_sample_size/2.0, prior_sample_size*prior_guess_for_variance/2.0)
tau = sqrt(1.0/prec_a)
# Set the prior for likelihood:
# Here we model the parameters b[1] and b[2], note these are constant
for (k in 1:2){
b[k] ~ dnorm(0.0, 1.0/1.0e6)
}
prior_sample_size_1 = 5
prior_guess_for_variance_1 = 10
prec ~ dgamma(prior_sample_size/2.0, prior_sample_size*prior_guess_for_variance/2.0)
# The output variable, to check variability
sig = sqrt(1.0/prec)
}"
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3) # bur-in period
mod_sim = coda.samples(model=mod, variable.names = params, n.iter = 5e3)
set.seed(116)
data_jags = list(y=dat$loginfant, log_income=dat$logincome, is_oil=as.numeric(dat$oil=="yes"), region=as.numeric(dat$region))
head(data_jags)
# Sanity Check: Since the oil variable is also a count variable, it is good to check if the oil exporting countries are all in the same region. If that is the case then we would never know if it was oil of region intercept that affected the variability in infant mortality (response variable). Howvere, thats not the case, so no problem, we just go ahead. It seems oil_expoting countries ar in region 1,2,3.
table(data_jags$is_oil, data_jags$region)
params = c("a0", "a", "b", "sig", "tau")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3) # bur-in period
mod_sim = coda.samples(model=mod, variable.names = params, n.iter = 5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
plot(mod_sim)
gelam.diag(mode_sim)
plot(mod_sim)
gelman.diag(mode_sim)
plot(mod_sim)
gelman.diag(mode_sim)
plot(mod_sim)
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
auto_corr.plot(mod_sim)
mod_string = "model{
# Note the intercept term a is separate for each region, while b[1] and b[2] are constant
for  (i in 1: length(y)){
y[i] ~ dnorm(mu[i], prec)
mu[i] = a[region[i]] + b[1]*log_income[i] + b[2]*is_oil[i]
}
# Here we model the parameter a for each region
for (j in 1:max(region)){
a[j] ~ dnorm(a0, prec_a)
}
# Now we model the hyperparameter for the intercept parameter a. We assume all a's come from the same distribution. This
# assumption is what makes hierarchial distribution different from modeling seperate models for each model with separate
# parameter. Our assumprtion of all a's comes from the same distribution is what makes hierarchial model share information
# between different group which is the case in the real world.
prior_sample_size = 1
prior_guess_for_variance = 10
a0 ~ dnorm(0.0, 1.0/1.0e6)            # Flat normal distribution [As good as non-informative prior]
prec_a ~ dgamma(prior_sample_size/2.0, prior_sample_size*prior_guess_for_variance/2.0)
tau = sqrt(1.0/prec_a)
# Set the prior for likelihood:
# Here we model the parameters b[1] and b[2], note these are constant
for (k in 1:2){
b[k] ~ dnorm(0.0, 1.0/1.0e6)
}
prior_sample_size_1 = 5
prior_guess_for_variance_1 = 10
prec ~ dgamma(prior_sample_size/2.0, prior_sample_size*prior_guess_for_variance/2.0)
# The output variable, to check variability
sig = sqrt(1.0/prec)
}"
set.seed(116)
data_jags = list(y=dat$loginfant, log_income=dat$logincome, is_oil=as.numeric(dat$oil=="yes"), region=as.numeric(dat$region))
head(data_jags)
# Sanity Check: Since the oil variable is also a count variable, it is good to check if the oil exporting countries are all in the same region. If that is the case then we would never know if it was oil of region intercept that affected the variability in infant mortality (response variable). Howvere, thats not the case, so no problem, we just go ahead. It seems oil_expoting countries ar in region 1,2,3.
table(data_jags$is_oil, data_jags$region)
params = c("a0", "a", "b", "sig", "tau")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3) # bur-in period
mod_sim = coda.samples(model=mod, variable.names = params, n.iter = 5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
plot(mod_sim)
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
auto_corr.plot(mod_sim)
plot(mod_sim)
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
effectiveSize(mod_sim)
dic.samples(mod, n.iter = 1e3)
summary(mod_sim)
data = read.csv("mixture.csv", headere=FALSE)
data_path = '/Users/sam/All-Program/App/Study/BayesianCompStats/mixture.csv'
data = read.csv("mixture.csv", headere=FALSE)
data_path = '/Users/sam/All-Program/App/Study/BayesianCompStats/mixture.csv'
data = read.csv(data_path, headere=FALSE)
data_path = '/Users/sam/All-Program/App/Study/BayesianCompStats/mixture.csv'
data = read.csv(data_path, header=FALSE)
head(data)
data_path = '/Users/sam/All-Program/App/Study/BayesianCompStats/mixture.csv'
data = read.csv(data_path, header=FALSE)
head(data)
y = data$V1
plot(density(y))
library("rjags")
model_string = "model{
for (i in 1:length(y)){
y[i] ~ dnorm(mu[z[i]], prec)
z[i] ~ dcat(omega)               # Since omega takes on two category we draw z's from categorical distribution
}
# Intuitively We saw that one mixture goes from -6 to 4 and the other goes from -4 to 6.
# We eexplicitely provide proper means such that the MCMC sample to quickly able to find the two mixture and distuingish them.
mu[1] ~ dnorm(-1.0, 1.0/100)
mu[2] ~ dnorm(1.0, 1.0100) T(mu[1])
# We provide the usual prior to the prec parameter which is again the variance
prior_sample_size_1 = 1
prior_guess_for_variance_1 = 1
prec ~ dgamma(prior_sample_size_1/2, prior_sample_size_1*prior_guess_for_variance_1/2)
# Now the important part comes in.
# Since omega is modeled categorically, we should provide it a prior distribution that follows the categorical norms.
# Beta or Dirichlet sound good for such cases. Lets use dirichlet
omega ~ ddrich(c(1.0,1.0))
}"
seet.seed(11)
set.seed(11)
data_jags = list(y)""
set.seed(11)
data_jags = list(y=y)
# We randomly choose few z's to monitor, such that the model is working fine
params = c("mu", "sig", "omega", "z[1]", "z[31]", "z[49]", "z[6]")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains = 3)
set.seed(11)
data_jags = list(y=y)
params = c("mu", "sig", "omega", "z[1]", "z[31]", "z[49]", "z[6]")
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains = 3)
data_jags
mod = jags.model(textConnection(mod_string), data=data_jags, n.chains = 3)
mod = jags.model(textConnection(model_string), data=data_jags, n.chains = 3)
model_string = "model{
for (i in 1:length(y)){
y[i] ~ dnorm(mu[z[i]], prec)
z[i] ~ dcat(omega)               # Since omega takes on two category we draw z's from categorical distribution
}
# Intuitively We saw that one mixture goes from -6 to 4 and the other goes from -4 to 6.
# We eexplicitely provide proper means such that the MCMC sample to quickly able to find the two mixture and distuingish them.
mu[1] ~ dnorm(-1.0, 1.0/100)
mu[2] ~ dnorm(1.0, 100.0) T(mu[1])
# We provide the usual prior to the prec parameter which is again the variance
prior_sample_size_1 = 1
prior_guess_for_variance_1 = 1
prec ~ dgamma(prior_sample_size_1/2, prior_sample_size_1*prior_guess_for_variance_1/2)
# Now the important part comes in.
# Since omega is modeled categorically, we should provide it a prior distribution that follows the categorical norms.
# Beta or Dirichlet sound good for such cases. Lets use dirichlet
omega ~ ddrich(c(1.0,1.0))
}"
mod = jags.model(textConnection(model_string), data=data_jags, n.chains = 3)
model_string = "model{
for (i in 1:length(y)){
y[i] ~ dnorm(mu[z[i]], prec)
z[i] ~ dcat(omega)               # Since omega takes on two category we draw z's from categorical distribution
}
# Intuitively We saw that one mixture goes from -6 to 4 and the other goes from -4 to 6.
# We eexplicitely provide proper means such that the MCMC sample to quickly able to find the two mixture and distuingish them.
mu[1] ~ dnorm(-1.0, 1.0/100)
mu[2] ~ dnorm(1.0, 100.0) T(mu[1],)
# We provide the usual prior to the prec parameter which is again the variance
prior_sample_size_1 = 1
prior_guess_for_variance_1 = 1
prec ~ dgamma(prior_sample_size_1/2, prior_sample_size_1*prior_guess_for_variance_1/2)
# Now the important part comes in.
# Since omega is modeled categorically, we should provide it a prior distribution that follows the categorical norms.
# Beta or Dirichlet sound good for such cases. Lets use dirichlet
omega ~ ddrich(c(1.0,1.0))
}"
mod = jags.model(textConnection(model_string), data=data_jags, n.chains = 3)
model_string = "model{
for (i in 1:length(y)){
y[i] ~ dnorm(mu[z[i]], prec)
z[i] ~ dcat(omega)               # Since omega takes on two category we draw z's from categorical distribution
}
# Intuitively We saw that one mixture goes from -6 to 4 and the other goes from -4 to 6.
# We eexplicitely provide proper means such that the MCMC sample to quickly able to find the two mixture and distuingish them.
mu[1] ~ dnorm(-1.0, 1.0/100)
mu[2] ~ dnorm(1.0, 100.0) T(mu[1],)
# We provide the usual prior to the prec parameter which is again the variance
prior_sample_size_1 = 1
prior_guess_for_variance_1 = 1
prec ~ dgamma(prior_sample_size_1/2, prior_sample_size_1*prior_guess_for_variance_1/2)
# Now the important part comes in.
# Since omega is modeled categorically, we should provide it a prior distribution that follows the categorical norms.
# Beta or Dirichlet sound good for such cases. Lets use dirichlet
omega ~ ddirich(c(1.0,1.0))
}"
mod = jags.model(textConnection(model_string), data=data_jags, n.chains = 3)
set.seed(11)
data_jags = list(y=y)
# We randomly choose few z's to monitor, such that the model is working fine
params = c("mu", "sig", "omega", "z[1]", "z[31]", "z[49]", "z[6]")
mod = jags.model(textConnection(model_string), data=data_jags, n.chains = 3)
# Burn-in period
update(mod, 1e3)
mod_sim = coda.samples(model = mod, variable.names = params, n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
model_string = "model{
for (i in 1:length(y)){
y[i] ~ dnorm(mu[z[i]], prec)
z[i] ~ dcat(omega)               # Since omega takes on two category we draw z's from categorical distribution
}
# Intuitively We saw that one mixture goes from -6 to 4 and the other goes from -4 to 6.
# We eexplicitely provide proper means such that the MCMC sample to quickly able to find the two mixture and distuingish them.
mu[1] ~ dnorm(-1.0, 1.0/100)
mu[2] ~ dnorm(1.0, 100.0) T(mu[1],)
# We provide the usual prior to the prec parameter which is again the variance
prior_sample_size_1 = 1
prior_guess_for_variance_1 = 1
prec ~ dgamma(prior_sample_size_1/2, prior_sample_size_1*prior_guess_for_variance_1/2)
sig = sqrt(1.0/prec)
# Now the important part comes in.
# Since omega is modeled categorically, we should provide it a prior distribution that follows the categorical norms.
# Beta or Dirichlet sound good for such cases. Lets use dirichlet
omega ~ ddirich(c(1.0,1.0))
}"
set.seed(11)
data_jags = list(y=y)
# We randomly choose few z's to monitor, such that the model is working fine
params = c("mu", "sig", "omega", "z[1]", "z[31]", "z[49]", "z[6]")
mod = jags.model(textConnection(model_string), data=data_jags, n.chains = 3)
# Burn-in period
update(mod, 1e3)
mod_sim = coda.samples(model = mod, variable.names = params, n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
plot(mod_sim)
summary(mod_sim)
require(stats)
require(graphics) # for time
ldeaths
mdeaths
require(graphics)
work <- diff(WWWusage)
require(stats)
aics <- matrix(, 6, 6, dimnames = list(p = 0:5, q = 0:5))
aics
or(q in 1:5) aics[1, 1+q] <- arima(WWWusage, c(0, 1, q),
for(q in 1:5) aics[1, 1+q] <- arima(WWWusage, c(0, 1, q),
optim.control = list(maxit = 500))$aic
for(p in 1:5)
for(q in 0:5) aics[1+p, 1+q] <- arima(WWWusage, c(p, 1, q),
optim.control = list(maxit = 500))$aic
round(aics - min(aics, na.rm = TRUE), 2)
require(stats);
require(graphics)
n <- length(dr <- c(VADeaths))
nam <- names(VADeaths)
(VADeaths)
summary(USArrests)
require(graphics)
USArrests
library("car")
data("Leinhardt")
Leinhardt
data("USAccDeaths")
USAccDeaths
library("MASS")
data("sensitivity")
load("/Users/sam/Downloads/multisensi/data/Climat.rda")
data("Climat")
a = load("/Users/sam/Downloads/multisensi/data/Climat.rda")
a
load("/Users/sam/Downloads/multisensi/data/biomasseY.rda")
b = load("/Users/sam/Downloads/multisensi/data/biomasseY.rda")
b
?load
a
a[1]
a["Climat"]
library(MASS)
data(mstate)
data(lme4)
library(sensitivity)
install.packages("sensitivity")
library(sensitivity)
data(climat)
data(Climat)
library(sensitivity)
data(Climat)
library(sensitivity) # to use fast99
library(sensitivity) # to use fast99
data(biomasseX)
data(Climat)
data(annualflows)
data(asthma)
require(stats)
dim(crimtab)
require(graphics)
pairs(airquality, panel = panel.smooth, main = "airquality data")
airquality
library("car")
data(Leinhardt)
Leinhardt
plot(mod1_sim)
outpath = '/Users/sam/All-Program/App-DataSet/DataScienceProjects/CreditCardFraud/'
dat = read.csv(paste(outpath,"sample_credit_fraud.csv"))
dim(dat)
head(dat)
ddexp = function(x, mu, tau){
0.5*tau*exp(-tau*abs(x-mu))
}
curve(ddexp(x, mu=0.0, tau=1.0), from=-10.0, to=10.0, ylab="density", main='Double exponential distribution')
library('rjags')
model_string = "model{
for (i in 1:length(Class)){
y[i] ~ dbern(p[i])
logit(p[i]) = b0 + b[1]*V1[i] + b[2]*V2[i] + b[3]*V3[i] + b[4]*V4[i] + b[5]*V5[i] + b[6]*V6[i] + b[7]*V7[i] + b[8]*V8[i] + b[9]*V9[i] + b[10]*V10[i] + b[11]*V11[i] + b[12]*V12[i] + b[12]*V12[i] + b[13]*V13[i] + b[14]*V14[i] + b[15]*V15[i] + b[16]*V16[i] + b[17]*V17[i] + b[18]*V18[i] + b[19]*V19[i] + b[20]*V20[i] + b[21]*V21[i] + b[22]*V22[i] + b[23]*V23[i] + b[24]*V24[i] + b[25]*V25[i] + b[26]*V26[i] + b[27]*V27[i] + b[28]*V28[i]
}
# We chouse priors for the parameters
b0 ~ dnorm(1.0, 1/25)               # Equivallent to a non-informative prior
for (j in 1:28){
b[j] ~ ddexp(0.0, sqrt(100.0))
}
}"
set.seed(912)
# Jags takes input as list
data_jags = list(Class=dat$Class, V1=dat[,'V1'], V2=dat[,'V2'], V3=dat[,'V3'], V4=dat[,'V4'], V5=dat[,'V5'], V6=dat[,'V6'], V7=dat[,'V7'], V8=dat[,'V8'], V9=dat[,'V9'], V10=dat[,'V10'], V11=dat[,'V11'], V12=dat[,'V12'], V13=dat[,'V13'], V14=dat[,'V14'], V15=dat[,'V15'], V16=dat[,'V16'], V17=dat[,'V17'], V18=dat[,'V18'], V19=dat[,'V19'], V20=dat[,'V20'], V21=dat[,'V21'], V22=dat[,'V22'], V23=dat[,'V23'], V24=dat[,'V24'], V25=dat[,'V25'], V26=dat[,'V26'],V27=dat[,'V27'], V28=dat[,'V28'])
# Parameters that we would wanna monitor
params = c('b0','b')
mod1 = jags.model(textConnection(model_string), data=data_jags, n.chains = 3)
# Burn-in period
update(mod1, 1e3)
mod1_sim = coda.samples(model=mod1, variable.names = params, n.iter = 5e3)
plot(mod1_sim)
plot(mod1_sim)
outpath = '/Users/sam/All-Program/App-DataSet/DataScienceProjects/CreditCardFraud/'
dat = read.csv(paste(outpath,"sample_credit_fraud.csv"))
dim(dat)
head(dat)
ddexp = function(x, mu, tau){
0.5*tau*exp(-tau*abs(x-mu))
}
curve(ddexp(x, mu=0.0, tau=1.0), from=-10.0, to=10.0, ylab="density", main='Double exponential distribution')
library('rjags')
model_string = "model{
for (i in 1:length(Class)){
y[i] ~ dbern(p[i])
logit(p[i]) = b0 + b[1]*V1[i] + b[2]*V2[i] + b[3]*V3[i] + b[4]*V4[i] + b[5]*V5[i] + b[6]*V6[i] + b[7]*V7[i] + b[8]*V8[i] + b[9]*V9[i] + b[10]*V10[i] + b[11]*V11[i] + b[12]*V12[i] + b[12]*V12[i] + b[13]*V13[i] + b[14]*V14[i] + b[15]*V15[i] + b[16]*V16[i] + b[17]*V17[i] + b[18]*V18[i] + b[19]*V19[i] + b[20]*V20[i] + b[21]*V21[i] + b[22]*V22[i] + b[23]*V23[i] + b[24]*V24[i] + b[25]*V25[i] + b[26]*V26[i] + b[27]*V27[i] + b[28]*V28[i]
}
# We chouse priors for the parameters
b0 ~ dnorm(1.0, 1/25)               # Equivallent to a non-informative prior
for (j in 1:28){
b[j] ~ ddexp(0.0, sqrt(2.0))
}
}"
set.seed(912)
# Jags takes input as list
data_jags = list(Class=dat$Class, V1=dat[,'V1'], V2=dat[,'V2'], V3=dat[,'V3'], V4=dat[,'V4'], V5=dat[,'V5'], V6=dat[,'V6'], V7=dat[,'V7'], V8=dat[,'V8'], V9=dat[,'V9'], V10=dat[,'V10'], V11=dat[,'V11'], V12=dat[,'V12'], V13=dat[,'V13'], V14=dat[,'V14'], V15=dat[,'V15'], V16=dat[,'V16'], V17=dat[,'V17'], V18=dat[,'V18'], V19=dat[,'V19'], V20=dat[,'V20'], V21=dat[,'V21'], V22=dat[,'V22'], V23=dat[,'V23'], V24=dat[,'V24'], V25=dat[,'V25'], V26=dat[,'V26'],V27=dat[,'V27'], V28=dat[,'V28'])
# Parameters that we would wanna monitor
params = c('b0','b')
mod1 = jags.model(textConnection(model_string), data=data_jags, n.chains = 3)
# Burn-in period
update(mod1, 1e3)
mod1_sim = coda.samples(model=mod1, variable.names = params, n.iter = 5e3)
plot(mod1_sim)
library(factoextra)
install.packages('factoextra)
library(factoextra)
install.packages('factoextra')
nstall.packages('factoextra')
install.packages('factoextra')
