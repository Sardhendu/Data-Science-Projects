{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Created by: Sardhendu Mishra\n",
    "# With Regards to: Course CS 584\n",
    "# CWID: A20388502\n",
    "######################################\n",
    "\n",
    "\n",
    "# import done here\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict, defaultdict\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading the Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1993, 104)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>communityname</th>\n",
       "      <th>fold</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>numbUrban</th>\n",
       "      <th>pctUrban</th>\n",
       "      <th>medIncome</th>\n",
       "      <th>pctWWage</th>\n",
       "      <th>pctWFarmSelf</th>\n",
       "      <th>pctWInvInc</th>\n",
       "      <th>pctWSocSec</th>\n",
       "      <th>pctWPubAsst</th>\n",
       "      <th>pctWRetire</th>\n",
       "      <th>medFamInc</th>\n",
       "      <th>perCapInc</th>\n",
       "      <th>whitePerCap</th>\n",
       "      <th>blackPerCap</th>\n",
       "      <th>indianPerCap</th>\n",
       "      <th>AsianPerCap</th>\n",
       "      <th>OtherPerCap</th>\n",
       "      <th>HispPerCap</th>\n",
       "      <th>NumUnderPov</th>\n",
       "      <th>PctPopUnderPov</th>\n",
       "      <th>PctLess9thGrade</th>\n",
       "      <th>PctNotHSGrad</th>\n",
       "      <th>PctBSorMore</th>\n",
       "      <th>PctUnemployed</th>\n",
       "      <th>PctEmploy</th>\n",
       "      <th>PctEmplManu</th>\n",
       "      <th>PctEmplProfServ</th>\n",
       "      <th>PctOccupManu</th>\n",
       "      <th>PctOccupMgmtProf</th>\n",
       "      <th>MalePctDivorce</th>\n",
       "      <th>MalePctNevMarr</th>\n",
       "      <th>FemalePctDiv</th>\n",
       "      <th>TotalPctDiv</th>\n",
       "      <th>PersPerFam</th>\n",
       "      <th>PctFam2Par</th>\n",
       "      <th>PctKids2Par</th>\n",
       "      <th>PctYoungKids2Par</th>\n",
       "      <th>PctTeen2Par</th>\n",
       "      <th>PctWorkMomYoungKids</th>\n",
       "      <th>PctWorkMom</th>\n",
       "      <th>NumIlleg</th>\n",
       "      <th>PctIlleg</th>\n",
       "      <th>NumImmig</th>\n",
       "      <th>PctImmigRecent</th>\n",
       "      <th>PctImmigRec5</th>\n",
       "      <th>PctImmigRec8</th>\n",
       "      <th>PctImmigRec10</th>\n",
       "      <th>PctRecentImmig</th>\n",
       "      <th>PctRecImmig5</th>\n",
       "      <th>PctRecImmig8</th>\n",
       "      <th>PctRecImmig10</th>\n",
       "      <th>PctSpeakEnglOnly</th>\n",
       "      <th>PctNotSpeakEnglWell</th>\n",
       "      <th>PctLargHouseFam</th>\n",
       "      <th>PctLargHouseOccup</th>\n",
       "      <th>PersPerOccupHous</th>\n",
       "      <th>PersPerOwnOccHous</th>\n",
       "      <th>PersPerRentOccHous</th>\n",
       "      <th>PctPersOwnOccup</th>\n",
       "      <th>PctPersDenseHous</th>\n",
       "      <th>PctHousLess3BR</th>\n",
       "      <th>MedNumBR</th>\n",
       "      <th>HousVacant</th>\n",
       "      <th>PctHousOccup</th>\n",
       "      <th>PctHousOwnOcc</th>\n",
       "      <th>PctVacantBoarded</th>\n",
       "      <th>PctVacMore6Mos</th>\n",
       "      <th>MedYrHousBuilt</th>\n",
       "      <th>PctHousNoPhone</th>\n",
       "      <th>PctWOFullPlumb</th>\n",
       "      <th>OwnOccLowQuart</th>\n",
       "      <th>OwnOccMedVal</th>\n",
       "      <th>OwnOccHiQuart</th>\n",
       "      <th>RentLowQ</th>\n",
       "      <th>RentMedian</th>\n",
       "      <th>RentHighQ</th>\n",
       "      <th>MedRent</th>\n",
       "      <th>MedRentPctHousInc</th>\n",
       "      <th>MedOwnCostPctInc</th>\n",
       "      <th>MedOwnCostPctIncNoMtg</th>\n",
       "      <th>NumInShelters</th>\n",
       "      <th>NumStreet</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabastercity</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AlexanderCitycity</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Annistoncity</td>\n",
       "      <td>3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Athenscity</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Auburncity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state      communityname  fold  population  householdsize  racepctblack  \\\n",
       "0      1      Alabastercity     7        0.01           0.61          0.21   \n",
       "1      1  AlexanderCitycity    10        0.01           0.41          0.55   \n",
       "2      1       Annistoncity     3        0.03           0.34          0.86   \n",
       "3      1         Athenscity     8        0.01           0.38          0.35   \n",
       "4      1         Auburncity     1        0.04           0.37          0.32   \n",
       "\n",
       "   racePctWhite  racePctAsian  racePctHisp  agePct12t21  agePct12t29  \\\n",
       "0          0.83          0.02         0.01         0.41         0.49   \n",
       "1          0.57          0.01         0.00         0.47         0.45   \n",
       "2          0.30          0.04         0.01         0.41         0.42   \n",
       "3          0.71          0.04         0.01         0.39         0.46   \n",
       "4          0.70          0.21         0.02         1.00         1.00   \n",
       "\n",
       "   agePct16t24  agePct65up  numbUrban  pctUrban  medIncome  pctWWage  \\\n",
       "0         0.26        0.21       0.02       1.0       0.46      0.77   \n",
       "1         0.31        0.57       0.00       0.0       0.15      0.33   \n",
       "2         0.27        0.59       0.04       1.0       0.12      0.32   \n",
       "3         0.31        0.49       0.00       0.0       0.23      0.43   \n",
       "4         1.00        0.14       0.05       1.0       0.02      0.77   \n",
       "\n",
       "   pctWFarmSelf  pctWInvInc  pctWSocSec  pctWPubAsst  pctWRetire  medFamInc  \\\n",
       "0          0.23        0.38        0.22         0.16        0.22       0.42   \n",
       "1          0.13        0.27        0.65         0.41        0.45       0.21   \n",
       "2          0.19        0.28        0.67         0.52        0.57       0.18   \n",
       "3          0.48        0.36        0.53         0.41        0.44       0.29   \n",
       "4          0.57        0.47        0.13         0.11        0.16       0.32   \n",
       "\n",
       "   perCapInc  whitePerCap  blackPerCap  indianPerCap  AsianPerCap  \\\n",
       "0       0.29         0.29         0.27          0.16         0.24   \n",
       "1       0.22         0.29         0.15          0.00         0.29   \n",
       "2       0.21         0.35         0.15          0.26         0.18   \n",
       "3       0.27         0.30         0.18          0.19         0.20   \n",
       "4       0.17         0.19         0.16          0.07         0.21   \n",
       "\n",
       "   OtherPerCap  HispPerCap  NumUnderPov  PctPopUnderPov  PctLess9thGrade  \\\n",
       "0         0.38        0.26         0.01            0.14             0.27   \n",
       "1         0.00        0.14         0.02            0.45             0.57   \n",
       "2         0.84        0.33         0.06            0.65             0.48   \n",
       "3         0.00        0.80         0.02            0.38             0.51   \n",
       "4         0.14        0.22         0.11            1.00             0.18   \n",
       "\n",
       "   PctNotHSGrad  PctBSorMore  PctUnemployed  PctEmploy  PctEmplManu  \\\n",
       "0          0.37         0.39           0.12       0.69         0.27   \n",
       "1          0.70         0.21           0.27       0.43         1.00   \n",
       "2          0.58         0.28           0.79       0.22         0.44   \n",
       "3          0.59         0.32           0.37       0.43         0.64   \n",
       "4          0.18         0.82           0.42       0.23         0.19   \n",
       "\n",
       "   PctEmplProfServ  PctOccupManu  PctOccupMgmtProf  MalePctDivorce  \\\n",
       "0             0.37          0.32              0.48            0.31   \n",
       "1             0.17          0.93              0.24            0.42   \n",
       "2             0.43          0.54              0.41            0.69   \n",
       "3             0.30          0.52              0.46            0.47   \n",
       "4             1.00          0.25              0.67            0.09   \n",
       "\n",
       "   MalePctNevMarr  FemalePctDiv  TotalPctDiv  PersPerFam  PctFam2Par  \\\n",
       "0            0.20          0.35         0.34        0.56        0.81   \n",
       "1            0.32          0.54         0.52        0.46        0.48   \n",
       "2            0.36          0.67         0.71        0.42        0.28   \n",
       "3            0.31          0.42         0.46        0.43        0.63   \n",
       "4            1.00          0.16         0.13        0.38        0.55   \n",
       "\n",
       "   PctKids2Par  PctYoungKids2Par  PctTeen2Par  PctWorkMomYoungKids  \\\n",
       "0         0.79              0.97         0.82                 0.60   \n",
       "1         0.43              0.48         0.59                 0.81   \n",
       "2         0.24              0.38         0.27                 0.49   \n",
       "3         0.62              0.66         0.53                 0.51   \n",
       "4         0.53              0.59         0.54                 0.68   \n",
       "\n",
       "   PctWorkMom  NumIlleg  PctIlleg  NumImmig  PctImmigRecent  PctImmigRec5  \\\n",
       "0        0.51      0.00      0.09      0.00            0.50          0.43   \n",
       "1        0.82      0.02      0.42      0.00            0.00          0.78   \n",
       "2        0.49      0.04      0.55      0.00            0.22          0.28   \n",
       "3        0.47      0.01      0.22      0.00            0.41          0.30   \n",
       "4        0.69      0.03      0.69      0.01            1.00          0.91   \n",
       "\n",
       "   PctImmigRec8  PctImmigRec10  PctRecentImmig  PctRecImmig5  PctRecImmig8  \\\n",
       "0          0.46           0.43            0.04          0.03          0.03   \n",
       "1          0.64           0.54            0.00          0.02          0.01   \n",
       "2          0.45           0.40            0.02          0.02          0.03   \n",
       "3          0.29           0.65            0.04          0.02          0.02   \n",
       "4          0.86           0.80            0.29          0.23          0.20   \n",
       "\n",
       "   PctRecImmig10  PctSpeakEnglOnly  PctNotSpeakEnglWell  PctLargHouseFam  \\\n",
       "0           0.02              0.97                 0.01             0.17   \n",
       "1           0.01              0.98                 0.02             0.19   \n",
       "2           0.03              0.95                 0.02             0.25   \n",
       "3           0.04              0.96                 0.02             0.19   \n",
       "4           0.17              0.90                 0.05             0.14   \n",
       "\n",
       "   PctLargHouseOccup  PersPerOccupHous  PersPerOwnOccHous  PersPerRentOccHous  \\\n",
       "0               0.21              0.65               0.64                0.56   \n",
       "1               0.18              0.43               0.43                0.42   \n",
       "2               0.22              0.36               0.34                0.41   \n",
       "3               0.18              0.38               0.43                0.32   \n",
       "4               0.07              0.25               0.41                0.22   \n",
       "\n",
       "   PctPersOwnOccup  PctPersDenseHous  PctHousLess3BR  MedNumBR  HousVacant  \\\n",
       "0             0.82              0.10            0.22       0.5        0.01   \n",
       "1             0.58              0.17            0.46       0.5        0.02   \n",
       "2             0.50              0.13            0.51       0.5        0.07   \n",
       "3             0.58              0.12            0.45       0.5        0.03   \n",
       "4             0.24              0.14            0.71       0.0        0.06   \n",
       "\n",
       "   PctHousOccup  PctHousOwnOcc  PctVacantBoarded  PctVacMore6Mos  \\\n",
       "0          0.85           0.82              0.00            0.26   \n",
       "1          0.73           0.58              0.18            0.40   \n",
       "2          0.56           0.52              0.31            0.60   \n",
       "3          0.66           0.55              0.18            0.26   \n",
       "4          0.66           0.21              0.07            0.24   \n",
       "\n",
       "   MedYrHousBuilt  PctHousNoPhone  PctWOFullPlumb  OwnOccLowQuart  \\\n",
       "0            0.83            0.13            0.66            0.16   \n",
       "1            0.56            0.55            0.48            0.06   \n",
       "2            0.40            0.55            0.33            0.05   \n",
       "3            0.58            0.42            0.41            0.10   \n",
       "4            0.77            0.27            0.10            0.16   \n",
       "\n",
       "   OwnOccMedVal  OwnOccHiQuart  RentLowQ  RentMedian  RentHighQ  MedRent  \\\n",
       "0          0.15           0.13      0.13        0.23       0.25     0.28   \n",
       "1          0.06           0.08      0.02        0.07       0.08     0.09   \n",
       "2          0.07           0.09      0.02        0.09       0.13     0.13   \n",
       "3          0.10           0.12      0.08        0.14       0.15     0.15   \n",
       "4          0.16           0.18      0.18        0.21       0.26     0.21   \n",
       "\n",
       "   MedRentPctHousInc  MedOwnCostPctInc  MedOwnCostPctIncNoMtg  NumInShelters  \\\n",
       "0               0.19              0.18                   0.25           0.00   \n",
       "1               0.20              0.16                   0.36           0.00   \n",
       "2               0.41              0.38                   0.47           0.01   \n",
       "3               0.24              0.17                   0.33           0.00   \n",
       "4               1.00              0.16                   0.24           0.00   \n",
       "\n",
       "   NumStreet  PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
       "0       0.00            0.03              0.70            0.40           0.34   \n",
       "1       0.00            0.00              0.93            0.66           0.82   \n",
       "2       0.02            0.04              0.77            0.59           0.70   \n",
       "3       0.00            0.03              0.78            0.56           0.67   \n",
       "4       0.00            0.12              0.49            0.12           0.00   \n",
       "\n",
       "   PctSameState85  LandArea  PopDens  PctUsePubTrans  LemasPctOfficDrugUn  \\\n",
       "0            0.57      0.05     0.06            0.01                  0.0   \n",
       "1            0.84      0.11     0.03            0.01                  0.0   \n",
       "2            0.64      0.06     0.11            0.04                  0.0   \n",
       "3            0.71      0.09     0.05            0.00                  0.0   \n",
       "4            0.15      0.09     0.09            0.01                  0.0   \n",
       "\n",
       "   ViolentCrimesPerPop  \n",
       "0                 0.06  \n",
       "1                 0.14  \n",
       "2                 1.00  \n",
       "3                 0.23  \n",
       "4                 0.15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOAD THE DATASET INTO PANDAS DATAFRAME:\n",
    "datadir = '/Users/sam/All-Program/App-DataSet/Data-Science-Projects/Crime-Prediction/communities-crime-clean.csv'\n",
    "\n",
    "crimeDF = pd.read_csv(datadir, sep=',', header='infer')\n",
    "\n",
    "# Displaying first five elements for all the columns\n",
    "print (crimeDF.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(crimeDF.head())\n",
    "    \n",
    "# Important feature variables\n",
    "nonPredictiveFeatures = ['state','communityname','fold']\n",
    "regTargetVar = ['ViolentCrimesPerPop']\n",
    "classTargetVar = ['highCrime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The cross validation Random state to be used is for all the model is;\n",
    "cv = model_selection.ShuffleSplit(n_splits=10,random_state=675) \n",
    "\n",
    "modelScores = defaultdict(list)\n",
    "modelTopFeatures = defaultdict(list)\n",
    "\n",
    "class ModelScoring():\n",
    "    \n",
    "    def __init__(self,modelName, modelObj):\n",
    "        modelScores[modelName] = []\n",
    "        modelTopFeatures[modelName] = []\n",
    "        self.modelName = modelName\n",
    "        self.model = modelObj\n",
    "        self.cv = model_selection.ShuffleSplit(n_splits=10,\n",
    "                                               random_state=675) \n",
    "        \n",
    "    def setData(self, dataIN, targetIN):\n",
    "        self.dataIN = dataIN\n",
    "        self.targetIN = targetIN\n",
    "        \n",
    "    def cvEvaluate(self, scoringMetric, cv=None):\n",
    "#         modelPerformaceMetric = OrderedDict()\n",
    "        \n",
    "        if cv:\n",
    "            self.cv = cv\n",
    "            \n",
    "        for metric in scoringMetric:\n",
    "            nFoldScore = model_selection.cross_val_score(self.model, \n",
    "                                                         self.dataIN, self.targetIN,\n",
    "                                                         cv=self.cv, \n",
    "                                                         scoring=metric)\n",
    "\n",
    "            modelScores[self.modelName].append({metric:np.mean(nFoldScore)})\n",
    "            \n",
    "    def evaluate(self, yTrue, yPred):\n",
    "        metric = metrics.precision_recall_fscore_support(yTrue, yPred)\n",
    "        modelScores[self.modelName].append({'precision':list(metric[0])})\n",
    "        modelScores[self.modelName].append({'recall':list(metric[1])})\n",
    "        modelScores[self.modelName].append({'fscore':list(metric[2])})\n",
    "    \n",
    "    def setTopFeatures(self, featureList, predictivePower):\n",
    "        for feature, weight in zip(featureList,predictivePower):\n",
    "            modelTopFeatures[self.modelName].append({feature:weight})\n",
    "    \n",
    "    \n",
    "    def getTopFeatures(self, num=5, featureWeights=None):\n",
    "        if not featureWeights:\n",
    "            featureWeights = self.model.coef_\n",
    "            \n",
    "        feauturePredictivePower = pd.DataFrame(columns=['highCrimePredictive_features',\n",
    "                                                      'high_featureWeight', 'lowCrimePredictive_features',\n",
    "                                                      'low_featureWeight'])\n",
    "\n",
    "        posTopWeight_index = featureWeights.flatten().argsort()[-num:][::-1]   # Find the indices of top most feature weights\n",
    "        negTopWeight_index = featureWeights.flatten().argsort()[:num]\n",
    "\n",
    "        posTopWeights = featureWeights.flatten()[posTopWeight_index]\n",
    "        negTopWeights = featureWeights.flatten()[negTopWeight_index]\n",
    "\n",
    "        high_CrimeFeatures = self.dataIN.columns[posTopWeight_index]\n",
    "        low_CrimeFeatures = self.dataIN.columns[negTopWeight_index]\n",
    "\n",
    "        feauturePredictivePower['highCrimePredictive_features'] = high_CrimeFeatures\n",
    "        feauturePredictivePower['high_featureWeight'] = posTopWeights\n",
    "        feauturePredictivePower['lowCrimePredictive_features'] = low_CrimeFeatures\n",
    "        feauturePredictivePower['low_featureWeight'] = negTopWeights\n",
    "#         print (high_CrimeFeatures+low_CrimeFeatures)\n",
    "#         print (posTopWeights+negTopWeights)\n",
    "        \n",
    "        self.setTopFeatures(featureList = [item for item in itertools.chain(high_CrimeFeatures, low_CrimeFeatures)], \n",
    "                            predictivePower = [item for item in itertools.chain(posTopWeights, negTopWeights)])\n",
    "        \n",
    "        return feauturePredictivePower\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Decision Trees:\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part (a):  Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Create a new field “highCrime” which is true if the crime rate per capita (ViolentCrimesPerPop) is greater than 0.1, and false otherwise. What are the percentage of positive and negative instances in the dataset?\n",
    "\n",
    "Solutions: The percentage of positive instances is 62.72 and the percentage of negative insances is 37.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape after adding the classification column is;  (1993, 105)\n",
      "Percent of positive instancs are 62.719518 and percent of negative instances are 37.280482 \n"
     ]
    }
   ],
   "source": [
    "crimeDF['highCrime'] = crimeDF['ViolentCrimesPerPop'] > 0.1\n",
    "print ('The shape after adding the classification column is; ', crimeDF.shape)\n",
    "percntgPositive = (sum(crimeDF['highCrime'] == True)/ (sum(crimeDF['highCrime'] == True) + sum(crimeDF['highCrime'] == False))) * 100\n",
    "percntgNegative = 100-percntgPositive\n",
    "print ('Percent of positive instancs are %f and percent of negative instances are %f '%(percntgPositive, percntgNegative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1993, 105)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>communityname</th>\n",
       "      <th>fold</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>...</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "      <th>highCrime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabastercity</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AlexanderCitycity</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Annistoncity</td>\n",
       "      <td>3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Athenscity</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Auburncity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   state      communityname  fold  population  householdsize  racepctblack  \\\n",
       "0      1      Alabastercity     7        0.01           0.61          0.21   \n",
       "1      1  AlexanderCitycity    10        0.01           0.41          0.55   \n",
       "2      1       Annistoncity     3        0.03           0.34          0.86   \n",
       "3      1         Athenscity     8        0.01           0.38          0.35   \n",
       "4      1         Auburncity     1        0.04           0.37          0.32   \n",
       "\n",
       "   racePctWhite  racePctAsian  racePctHisp  agePct12t21    ...      \\\n",
       "0          0.83          0.02         0.01         0.41    ...       \n",
       "1          0.57          0.01         0.00         0.47    ...       \n",
       "2          0.30          0.04         0.01         0.41    ...       \n",
       "3          0.71          0.04         0.01         0.39    ...       \n",
       "4          0.70          0.21         0.02         1.00    ...       \n",
       "\n",
       "   PctBornSameState  PctSameHouse85  PctSameCity85  PctSameState85  LandArea  \\\n",
       "0              0.70            0.40           0.34            0.57      0.05   \n",
       "1              0.93            0.66           0.82            0.84      0.11   \n",
       "2              0.77            0.59           0.70            0.64      0.06   \n",
       "3              0.78            0.56           0.67            0.71      0.09   \n",
       "4              0.49            0.12           0.00            0.15      0.09   \n",
       "\n",
       "   PopDens  PctUsePubTrans  LemasPctOfficDrugUn  ViolentCrimesPerPop  \\\n",
       "0     0.06            0.01                  0.0                 0.06   \n",
       "1     0.03            0.01                  0.0                 0.14   \n",
       "2     0.11            0.04                  0.0                 1.00   \n",
       "3     0.05            0.00                  0.0                 0.23   \n",
       "4     0.09            0.01                  0.0                 0.15   \n",
       "\n",
       "   highCrime  \n",
       "0      False  \n",
       "1       True  \n",
       "2       True  \n",
       "3       True  \n",
       "4       True  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (crimeDF.shape)\n",
    "crimeDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part (b):  Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of dataForDT after preprocessing is:  (1993, 100)\n",
      "The shape of targetForDT after preprocessing is:  (1993,)\n",
      "[0 1 1 ..., 1 1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>NumStreet</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0        0.01           0.61          0.21          0.83          0.02   \n",
       "1        0.01           0.41          0.55          0.57          0.01   \n",
       "2        0.03           0.34          0.86          0.30          0.04   \n",
       "3        0.01           0.38          0.35          0.71          0.04   \n",
       "4        0.04           0.37          0.32          0.70          0.21   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  \\\n",
       "0         0.01         0.41         0.49         0.26        0.21   \n",
       "1         0.00         0.47         0.45         0.31        0.57   \n",
       "2         0.01         0.41         0.42         0.27        0.59   \n",
       "3         0.01         0.39         0.46         0.31        0.49   \n",
       "4         0.02         1.00         1.00         1.00        0.14   \n",
       "\n",
       "          ...           NumStreet  PctForeignBorn  PctBornSameState  \\\n",
       "0         ...                0.00            0.03              0.70   \n",
       "1         ...                0.00            0.00              0.93   \n",
       "2         ...                0.02            0.04              0.77   \n",
       "3         ...                0.00            0.03              0.78   \n",
       "4         ...                0.00            0.12              0.49   \n",
       "\n",
       "   PctSameHouse85  PctSameCity85  PctSameState85  LandArea  PopDens  \\\n",
       "0            0.40           0.34            0.57      0.05     0.06   \n",
       "1            0.66           0.82            0.84      0.11     0.03   \n",
       "2            0.59           0.70            0.64      0.06     0.11   \n",
       "3            0.56           0.67            0.71      0.09     0.05   \n",
       "4            0.12           0.00            0.15      0.09     0.09   \n",
       "\n",
       "   PctUsePubTrans  LemasPctOfficDrugUn  \n",
       "0            0.01                  0.0  \n",
       "1            0.01                  0.0  \n",
       "2            0.04                  0.0  \n",
       "3            0.00                  0.0  \n",
       "4            0.01                  0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the label into proper binary labels\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Remove the non-predictive and the target column form the data\n",
    "dataContinuous = crimeDF.drop(nonPredictiveFeatures+regTargetVar+classTargetVar, 1)  \n",
    "# Convert the target column into binomial label vector\n",
    "targetBinomial = lb.fit_transform(crimeDF['highCrime']).flatten()      \n",
    "\n",
    "print ('The shape of dataForDT after preprocessing is: ', dataContinuous.shape)\n",
    "print ('The shape of targetForDT after preprocessing is: ', targetBinomial.shape)\n",
    "# print (dataForDT)\n",
    "print (targetBinomial)\n",
    "dataContinuous.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### i. What are the training accuracy, precision, and recall for this tree? \n",
    "\n",
    "Solution: \n",
    "\n",
    "Accuracy = 1.0, Precision = 1.0, Recall = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'precision': [1.0, 1.0]}, {'recall': [1.0, 1.0]}, {'fscore': [1.0, 1.0]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Decision tree classifier:\n",
    "classifierDT = tree.DecisionTreeClassifier()\n",
    "classifierDT = classifierDT.fit(dataContinuous, targetBinomial)\n",
    "\n",
    "# Predict on the training Data\n",
    "predForDT = classifierDT.predict(dataContinuous)\n",
    "\n",
    "# ModelScoring\n",
    "modelName = 'Decision Tree Classifier'\n",
    "scoreObj = ModelScoring(modelName = modelName, modelObj=classifierDT)\n",
    "scoreObj.evaluate(targetBinomial,predForDT)\n",
    "modelScores[modelName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the decision tree classifier is:  1.0\n",
      "The precision of the decision tree classifier is:  1.0\n",
      "The recall of the decision tree classifier is:  1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0     1\n",
       "row_0           \n",
       "0      743     0\n",
       "1        0  1250"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Precision and Recall\n",
    "cnfMatrix = pd.crosstab(targetBinomial, predForDT)\n",
    "cnfMatrix\n",
    "tpDT =  cnfMatrix.iloc[1][1]\n",
    "tnDT =  cnfMatrix.iloc[0][0]\n",
    "fpDT =  cnfMatrix.iloc[0][1]\n",
    "fnDT =  cnfMatrix.iloc[1][0]\n",
    "print ('The accuracy of the decision tree classifier is: ', (tpDT+tnDT)/(tpDT+tnDT+fpDT+fnDT))\n",
    "print ('The precision of the decision tree classifier is: ', tpDT/(tpDT+fpDT))\n",
    "print ('The recall of the decision tree classifier is: ', tpDT/(tpDT+fnDT))\n",
    "# cnfMatrix.iloc[0][0] + cnfMatrix.iloc[1][1]/(cnfMatrix.iloc[0][0] + cnfMatrix.iloc[1][1] + cnfMatrix.iloc[0][0] + cnfMatrix.iloc[1][1])\n",
    "cnfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ii. What are the main features used for classification? Can you explain why they make sense (or not)?\n",
    "\n",
    "Solution: The top five features for classification are:\n",
    "\n",
    "PctKids2Par : Percentage of kids in family housing with two parents : This feature makes sense as a negative correlation to the dependent variable because a perpetrator would be reluctant on commiting violent crimes in front of a child. The more the house is of childrens the less would be the proximity of violent crimes.\n",
    "\n",
    "racePctWhite : percentage of population that is caucasian: This feature makes sense again as a negative correlation to the dependent variable because its evedent that the more the percentage of people are white the less the crime is. Evidently more proportion of white people are employed and educated, and in general educated and emlployed people have very less probability of commiting violent crime.\n",
    "\n",
    "PctLess9thGrade: percentage of people 25 and over with less than a 9th grade education: People with less than 9th grade and with a age of more than 25% sould incomparison engage in more crime/violent crime because education is directly propotional to employment and people nature and attitude. Employed people with genial nature would seldom engage in crime.\n",
    "\n",
    "PctEmplManu : percentage of people 16 and over who are employed in manufacturing\n",
    "\n",
    "blackPerCap : per capita income for african americans: This variable would again have a negative correlationship as stated above employed people with a descent salary would seldom engage in violent crime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 5 features selected by decision tree are: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>infoGain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>PctKids2Par</td>\n",
       "      <td>0.360083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>racePctWhite</td>\n",
       "      <td>0.088177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>racePctHisp</td>\n",
       "      <td>0.047067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PctLess9thGrade</td>\n",
       "      <td>0.020824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PctEmplManu</td>\n",
       "      <td>0.017148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>PctHousOwnOcc</td>\n",
       "      <td>0.014166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>HousVacant</td>\n",
       "      <td>0.013431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>HispPerCap</td>\n",
       "      <td>0.013414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>MedRent</td>\n",
       "      <td>0.013368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AsianPerCap</td>\n",
       "      <td>0.013347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_name  infoGain\n",
       "44      PctKids2Par  0.360083\n",
       "3      racePctWhite  0.088177\n",
       "5       racePctHisp  0.047067\n",
       "29  PctLess9thGrade  0.020824\n",
       "34      PctEmplManu  0.017148\n",
       "73    PctHousOwnOcc  0.014166\n",
       "71       HousVacant  0.013431\n",
       "26       HispPerCap  0.013414\n",
       "85          MedRent  0.013368\n",
       "24      AsianPerCap  0.013347"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best Fetures: Information gain with GINI\n",
    "ImpfeatureDF = pd.DataFrame(columns=['feature_name', 'infoGain'])\n",
    "featureVec = np.array(dataContinuous.columns)\n",
    "ImpfeatureDF['feature_name'] = np.array(dataContinuous.columns)\n",
    "# print (featureVec)\n",
    "ImpfeatureDF['infoGain'] = classifierDT.feature_importances_\n",
    "\n",
    "print ('The Top 5 features selected by decision tree are: ')\n",
    "inpFeatue = ImpfeatureDF.sort_values('infoGain', ascending=False).head(10)\n",
    "\n",
    "scoreObj.setTopFeatures(inpFeatue['feature_name'], inpFeatue['infoGain'])\n",
    "inpFeatue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part (c): Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### i. What are the 10-fold cross-validation accuracy, precision, and recall?\n",
    "\n",
    "Solution: mean 10-fold Accuracy = 0.7755, mean 10-fold Precision = 0.8016 and mean 10-fold Recall = 0.83343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.77299999999999991},\n",
       " {'precision': 0.80138770150926086},\n",
       " {'recall': 0.82182553257930235}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import random\n",
    "# random.seed(3213)\n",
    "classifierDT = tree.DecisionTreeClassifier()\n",
    "\n",
    "modelName = 'Decision Tree cv Classifier'\n",
    "scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierDT)\n",
    "scoreOBJ.setData(dataIN=dataContinuous, targetIN=targetBinomial)\n",
    "scoreOBJ.cvEvaluate(scoringMetric=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "modelScores[modelName]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ii. Why are they different from the results in the previous test?\n",
    "\n",
    "Solution:\n",
    "In the first Decision Tree fit, since we fit and classify on the whole dataset, the tree learned the entire data and made very good classification with 100% accuracy, precision and recall. This states that the previous method is prone to overfitting and may not generalize well for a new data, as data in the real world are always noisy. In the current model since we do crossvalidation we train on separate data and test on separate data. The model performs poorly on the test data and hence the accuracy. precision and recall of the model decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Linear Classification:\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part (a) :-  GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### i. What is the 10-fold cross-validation accuracy, precision, and recall for this method?\n",
    "\n",
    "Solutions: mean n-fold Accuracy = 0.7835, mean n-fold Precision = 0.9312 and mean Recall = 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.78350000000000009},\n",
       " {'precision': 0.93122871464730816},\n",
       " {'recall': 0.69799123705957622}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifierGNB = GaussianNB()\n",
    "# classifierGNB.fit()\n",
    "\n",
    "modelName = 'Gaussian Naive Bayes Classifier'\n",
    "scoreGNB = ModelScoring(modelName = modelName, modelObj = classifierGNB)\n",
    "scoreGNB.setData(dataIN=dataContinuous, targetIN=targetBinomial)\n",
    "scoreGNB.cvEvaluate(scoringMetric=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "modelScores[modelName]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ii. What are the 10 most predictive features? This can be measured by the normalized absolute difference of means for the feature between the two classes:\n",
    "\n",
    "Solution: The 10 most predictive features are given below.\n",
    "\n",
    "\n",
    "PctKids2Par: As discussed above this feature has a negative weight which means that it is mostely negative correlated to the dependent variable. The more the house is of childrens the less would be the proximity of violent crimes. As perpetratos would be more reluctant in commiting violent crime in a house full of childrens.\n",
    "\n",
    "PctFam2Par: percentage of families (with kids) that are headed by two parents\n",
    "\n",
    "racePctWhite : This feature has a negative weight which indicates a negative correlation with the dependent variable. Evidently more proportion of white people are employed and educated, and in general educated and emlployed people have very less probability of commiting violent crime.\n",
    "\n",
    "PctIlleg: percentage of kids born to never married; \n",
    "\n",
    "FemalePctDiv: percentage of females who are divorced\n",
    "\n",
    "TotalPctDiv\n",
    "\n",
    "PctYoungKids2Par\n",
    "\n",
    "pctWInvInc\n",
    "\n",
    "PctTeen2Par\n",
    "\n",
    "MalePctDivorce\n",
    "\n",
    "\n",
    "---> The larger this different, the more predictive the feature. Why do these make sense?\n",
    "This makes sense because each feature would have a mean value for corresponding to each class. If the difference between the mean is big, this indicated that the feature provides a very good seperation between the different class. The denominator indicates a lower standard deviation is proportional to higher predictive power. This makes sense because if the standard deviation is large, the gaussian distribution would result in a shorter and wider curve. For a particular feature, this indicates that the data is widely distributed and there is high chance that the two distribution corresponding to two class might intersect, which may result in bad separation between the different clss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>PredictivePower</th>\n",
       "      <th>NegPos-Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>PctKids2Par</td>\n",
       "      <td>0.809748</td>\n",
       "      <td>-0.809748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>PctFam2Par</td>\n",
       "      <td>0.745545</td>\n",
       "      <td>-0.745545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>racePctWhite</td>\n",
       "      <td>0.73523</td>\n",
       "      <td>-0.73523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>PctIlleg</td>\n",
       "      <td>0.709261</td>\n",
       "      <td>0.709261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>FemalePctDiv</td>\n",
       "      <td>0.693978</td>\n",
       "      <td>0.693978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TotalPctDiv</td>\n",
       "      <td>0.674645</td>\n",
       "      <td>0.674645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>PctYoungKids2Par</td>\n",
       "      <td>0.665009</td>\n",
       "      <td>-0.665009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pctWInvInc</td>\n",
       "      <td>0.661076</td>\n",
       "      <td>-0.661076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>PctTeen2Par</td>\n",
       "      <td>0.642949</td>\n",
       "      <td>-0.642949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MalePctDivorce</td>\n",
       "      <td>0.616864</td>\n",
       "      <td>0.616864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature_name PredictivePower NegPos-Coefficient\n",
       "44       PctKids2Par        0.809748          -0.809748\n",
       "43        PctFam2Par        0.745545          -0.745545\n",
       "3       racePctWhite         0.73523           -0.73523\n",
       "50          PctIlleg        0.709261           0.709261\n",
       "40      FemalePctDiv        0.693978           0.693978\n",
       "41       TotalPctDiv        0.674645           0.674645\n",
       "45  PctYoungKids2Par        0.665009          -0.665009\n",
       "15        pctWInvInc        0.661076          -0.661076\n",
       "46       PctTeen2Par        0.642949          -0.642949\n",
       "38    MalePctDivorce        0.616864           0.616864"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexClassNo = np.where(targetBinomial==0)[0]\n",
    "indexClassYes = np.where(targetBinomial==1)[0]\n",
    "# print (indexClassYes)\n",
    "\n",
    "NBfeauturePredictivePower = pd.DataFrame(columns=['feature_name','PredictivePower','NegPos-Coefficient'])\n",
    "NBfeauturePredictivePower['feature_name'] = np.array(dataContinuous.columns)\n",
    "\n",
    "for num, features in enumerate(dataContinuous.columns):\n",
    "#     print (features)\n",
    "    meanNo = np.mean(dataContinuous[features][indexClassNo])\n",
    "    meanYes = np.mean(dataContinuous[features][indexClassYes])\n",
    "    stdNo = np.std(dataContinuous[features][indexClassNo])\n",
    "    stdYes = np.std(dataContinuous[features][indexClassYes])\n",
    "    \n",
    "    predictivePower = np.abs(meanYes - meanNo)/(stdYes + stdNo)\n",
    "    negPosCoefficient = (meanYes - meanNo)/(stdYes + stdNo)\n",
    "    NBfeauturePredictivePower.ix[num, 'PredictivePower'] = predictivePower\n",
    "    NBfeauturePredictivePower.ix[num, 'NegPos-Coefficient'] = negPosCoefficient\n",
    "\n",
    "\n",
    "topFeature = NBfeauturePredictivePower.sort_values('PredictivePower', ascending=False).head(10)\n",
    "scoreGNB.setTopFeatures(topFeature['feature_name'], topFeature['NegPos-Coefficient'])\n",
    "\n",
    "topFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### iii. How do these results compare with your results from decision trees, above?\n",
    "\n",
    "Sol: Feature \"PctKids2Par\" is treated as the most significant feature by both the algorithms. Also, \"racePctWhite\" is treated important by both the algorithms. All other features are different in both the cases. The difference is majorly because decision tree choses fits fetures on a forward selection basis and in doing so its choice for the next feature highly depends on the previously chosen feature, which introduces a sense of correlation between the features. In Gaussian Naive bayes the features are trated independent given the class. \n",
    "\n",
    "For example seeing the top 10 in both the case. \n",
    "\n",
    "For a decision tree case \"PctEmploy\", \"PctLess9thGrade\", \"PctEmplManu\", can be thought of highly correlated to each other because they talk about education or emloyment. Also features \"PctWOFullPlumb\", \"HousVacant\", \"PctVacMore6Mos\" are also significantly correlated as houses with less plumbing would be less occupied and so on. However, in the case of Gaussian Naive Bayes the features are seldom related, It seems more random than the feature selection in Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part (b) :-  LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### i. What is the 10-fold cross-validation accuracy, precision, and recall for this method?\n",
    "\n",
    "Solutions: mean n-fold Accuracy = 0.839, mean n-fold Precision = 0.864348097316 and mean Recall = 0.87395706326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.83900000000000008},\n",
       " {'precision': 0.86434809731621909},\n",
       " {'recall': 0.8739570632604845}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "classifierSVC = LinearSVC()\n",
    "# classifierGNB.fit()\n",
    "\n",
    "modelName = 'Linear SVC Classifier'\n",
    "scoreLSVC = ModelScoring(modelName = modelName, modelObj = classifierSVC)\n",
    "scoreLSVC.setData(dataIN=dataContinuous, targetIN=targetBinomial)\n",
    "scoreLSVC.cvEvaluate(scoringMetric=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "modelScores[modelName]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ii. What are the 10 most predictive features? This can be measured by the absolute feature weights in the model. Why do these make sense (or not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>absFeatureWeights</th>\n",
       "      <th>featureWeights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pctWInvInc</td>\n",
       "      <td>1.888487</td>\n",
       "      <td>-1.888487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>PersPerOccupHous</td>\n",
       "      <td>1.755123</td>\n",
       "      <td>1.755123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>racePctWhite</td>\n",
       "      <td>1.500220</td>\n",
       "      <td>-1.500220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>PctKids2Par</td>\n",
       "      <td>1.190328</td>\n",
       "      <td>-1.190328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>RentHighQ</td>\n",
       "      <td>1.066884</td>\n",
       "      <td>1.066884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MalePctDivorce</td>\n",
       "      <td>1.065696</td>\n",
       "      <td>1.065696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NumUnderPov</td>\n",
       "      <td>1.051548</td>\n",
       "      <td>1.051548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NumStreet</td>\n",
       "      <td>1.019155</td>\n",
       "      <td>1.019155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>PctOccupMgmtProf</td>\n",
       "      <td>1.014675</td>\n",
       "      <td>1.014675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>population</td>\n",
       "      <td>1.002301</td>\n",
       "      <td>1.002301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature_name  absFeatureWeights  featureWeights\n",
       "15        pctWInvInc           1.888487       -1.888487\n",
       "64  PersPerOccupHous           1.755123        1.755123\n",
       "3       racePctWhite           1.500220       -1.500220\n",
       "44       PctKids2Par           1.190328       -1.190328\n",
       "84         RentHighQ           1.066884        1.066884\n",
       "38    MalePctDivorce           1.065696        1.065696\n",
       "27       NumUnderPov           1.051548        1.051548\n",
       "90         NumStreet           1.019155        1.019155\n",
       "37  PctOccupMgmtProf           1.014675        1.014675\n",
       "0         population           1.002301        1.002301"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierSVC.fit(dataContinuous, targetBinomial)\n",
    "\n",
    "SVCfeauturePredictivePower = pd.DataFrame(columns=['feature_name','absFeatureWeights', 'featureWeights'])\n",
    "SVCfeauturePredictivePower['feature_name'] = np.array(dataContinuous.columns)\n",
    "SVCfeauturePredictivePower['absFeatureWeights'] = np.abs(classifierSVC.coef_).flatten()\n",
    "SVCfeauturePredictivePower['featureWeights'] = classifierSVC.coef_.flatten()\n",
    "topFeatures = SVCfeauturePredictivePower.sort_values('absFeatureWeights', ascending=False).head(10)\n",
    "\n",
    "scoreLSVC.setTopFeatures(topFeatures['feature_name'], topFeatures['featureWeights'])\n",
    "\n",
    "topFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### iii. How do these results compare with your results from decision trees, above\n",
    "\n",
    "Sol: The feature selection in linear SVC is very different from decision trees in the sense that feature selection in decision tree depends higly on the first feature selected.\n",
    "\n",
    "Only \"racePctWhite\" managed to get into the top 10 features for both the algorithms. however the feature selection in linearSVC has many interesting features such as \"Population\", \"PersPerOccupHours\", and \"RentHighQ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Regression:\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part (a) : Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### i. Using 10-fold cross-validation, what is the estimated mean-squared-error (MSE) of the model?\n",
    "\n",
    "Sol: Estimated MSE 10-fold: 0.0189222775959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg_mean_squared_error': -0.018922277595922486}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data to fit linear model, To fit the linear model the data would be the same, however the target would change to continuous Variable\n",
    "# Remove the non-predictive and the target column form the data\n",
    "targetContinuous = crimeDF[regTargetVar]\n",
    "\n",
    "# Fit a linear model for cross vlidation:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "classifierLR = LinearRegression()\n",
    "\n",
    "modelName='Linear Regression Classifier'\n",
    "scoreLR = ModelScoring(modelName = modelName, modelObj = classifierLR)\n",
    "scoreLR.setData(dataIN=dataContinuous, targetIN=targetContinuous)\n",
    "scoreLR.cvEvaluate(scoringMetric=['neg_mean_squared_error'])\n",
    "\n",
    "modelScores[modelName]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ii. What is the MSE on the training set (train on all the data then test on it all)?\n",
    "\n",
    "Sol: 0.0165167748803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on the training set for the Linear model is : 0.0165167748803\n"
     ]
    }
   ],
   "source": [
    "classifierLR.fit(dataContinuous, targetContinuous)\n",
    "predictContinuousLR = classifierLR.predict(dataContinuous).flatten()\n",
    "mseLR = metrics.mean_squared_error(targetContinuous, predictContinuousLR)\n",
    "\n",
    "print ('The MSE on the training set for the Linear model is :', mseLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### iii. What features are most predictive of a high crime rate? A low crime rate?\n",
    "\n",
    "Sol: \n",
    "\n",
    "HighCrime : PersPerOccupation, PctHousownOcc, MalePctDivorce, PctRecImmig8, MedRent\n",
    "\n",
    "LowCrime: PctPersOwnOccup, TotalPctDiv, whitePerCap, PctKids2Par, OwnOccLowQuart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highCrimePredictive_features</th>\n",
       "      <th>high_featureWeight</th>\n",
       "      <th>lowCrimePredictive_features</th>\n",
       "      <th>low_featureWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PersPerOccupHous</td>\n",
       "      <td>0.635088</td>\n",
       "      <td>PctPersOwnOccup</td>\n",
       "      <td>-0.675694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PctHousOwnOcc</td>\n",
       "      <td>0.568133</td>\n",
       "      <td>TotalPctDiv</td>\n",
       "      <td>-0.561924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MalePctDivorce</td>\n",
       "      <td>0.458517</td>\n",
       "      <td>whitePerCap</td>\n",
       "      <td>-0.351016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PctRecImmig8</td>\n",
       "      <td>0.432511</td>\n",
       "      <td>PctKids2Par</td>\n",
       "      <td>-0.322651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MedRent</td>\n",
       "      <td>0.372728</td>\n",
       "      <td>OwnOccLowQuart</td>\n",
       "      <td>-0.308170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  highCrimePredictive_features  high_featureWeight  \\\n",
       "0             PersPerOccupHous            0.635088   \n",
       "1                PctHousOwnOcc            0.568133   \n",
       "2               MalePctDivorce            0.458517   \n",
       "3                 PctRecImmig8            0.432511   \n",
       "4                      MedRent            0.372728   \n",
       "\n",
       "  lowCrimePredictive_features  low_featureWeight  \n",
       "0             PctPersOwnOccup          -0.675694  \n",
       "1                 TotalPctDiv          -0.561924  \n",
       "2                 whitePerCap          -0.351016  \n",
       "3                 PctKids2Par          -0.322651  \n",
       "4              OwnOccLowQuart          -0.308170  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topFeatureDF = scoreLR.getTopFeatures(num=5)\n",
    "topFeatureDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part (b) : Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### i. What is the estimated MSE of the model under 10-fold CV?\n",
    "\n",
    "Sol: 0.01858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg_mean_squared_error': -0.018581731506130121}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "alpha = np.array([10, 1, 0.1, 0.01, 0.001], dtype='float64')\n",
    "\n",
    "classifierRR = RidgeCV(alpha)\n",
    "\n",
    "modelName='Ridge Regression Classifier'\n",
    "scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierRR)\n",
    "scoreOBJ.setData(dataIN=dataContinuous, targetIN=targetContinuous)\n",
    "scoreOBJ.cvEvaluate(scoringMetric=['neg_mean_squared_error'])\n",
    "\n",
    "modelScores[modelName]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ii. What is the MSE on the training set (train on all the data then test on it all)?\n",
    "\n",
    "Sol: 0.0167635291552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for the Ridge Regression model with default parameter setting is:  0.0167635291552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# classifierRR = Ridge()\n",
    "classifierRR = classifierRR.fit(dataContinuous, targetContinuous)\n",
    "predictionContinuousRR = classifierRR.predict(dataContinuous)\n",
    "\n",
    "mseRR = metrics.mean_squared_error(targetContinuous, predictionContinuousRR)\n",
    "print ('The MSE for the Ridge Regression model with default parameter setting is: ', mseRR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### iii. What is the best alpha?\n",
    "\n",
    "Sol: The best alpha value is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierRR.fit(dataContinuous, targetContinuous)\n",
    "classifierRR.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### iv. What does this say about the amount of overfitting in linear regression for this problem?\n",
    "\n",
    "Solution: The alpha value of 1 is found to be the best alpha using Cross Validation Score. This means we are neither enforcing strong regularization nor are be enforcing strong likelihood. But we are still penalizing the model using L2 norm. This also means that the model with only the likelihood is not very highly prone to overfitting but is still a little bit prone to overfitting, since we are penalizing the likelihood by weighting the regularization term by alpha=1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part (c) : Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### i. What is the estimated MSE of the model under 10-fold CV?\n",
    "\n",
    "The mean MSE for 10-fold cross validation (Polynomial Regression with L2 norm) is 0.017610408756280137\n",
    "\n",
    "The mean MSE for 10-fold cross validation (Polynomial Regression) is 0.100727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the 2nd order dataset is:  (1993, 5150)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'neg_mean_squared_error': -0.017610408756280137}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Get the Polynomial Features Data\n",
    "classifierPR = PolynomialFeatures(degree=2, include_bias=False)\n",
    "dataContinuousPoly = classifierPR.fit_transform(dataContinuous)\n",
    "print ('The shape of the 2nd order dataset is: ', dataContinuousPoly.shape)\n",
    "\n",
    "classifierLR_poly = LinearRegression()\n",
    "classifierRR_Poly = RidgeCV(alpha)\n",
    "\n",
    "modelName='Polynomial Ridge Regression Classifier'\n",
    "scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierRR_Poly)\n",
    "scoreOBJ.setData(dataIN=dataContinuousPoly, targetIN=targetContinuous)\n",
    "scoreOBJ.cvEvaluate(scoringMetric=['neg_mean_squared_error'])\n",
    "\n",
    "modelScores[modelName]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ii. What is the MSE on the training set (train on all the data then test on it all)?\n",
    "\n",
    "Sol: The mean squared error (polynomial features) of the linear regression model is 1.09352811987e-28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (polynomial features) of the linear regression model is 1.09352811987e-28 : \n"
     ]
    }
   ],
   "source": [
    "classifierLR_poly = classifierLR_poly.fit(dataContinuousPoly, targetContinuous)\n",
    "predictContinuousLR_poly = classifierLR_poly.predict(dataContinuousPoly).flatten()\n",
    "\n",
    "mseLR_poly = metrics.mean_squared_error(targetContinuous, predictContinuousLR_poly)\n",
    "\n",
    "print ('The mean squared error (polynomial features) of the linear regression model is %s : '%(str(mseLR_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### iii. Does this mean the quadratic model is better than the linear model for this problem?\n",
    "\n",
    "Sol: Despite the MSE of the polynomial feature space (2nd order) is much closer to 0, the quadratic model is not a good fit. Because, since we train and test on the same dataset, the model learns the training data very well. This scenario may lead to overfitting as the real world dataset is more noisy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dirty Data\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part (a):  Repeat the decision tree learning question for the full (non-clean) data set and present the results. Are the CV results better or worse? What does this say about the effect of missing values?\n",
    "\n",
    "Solution: \n",
    "\n",
    "Clean Data: mean 10-fold Accuracy = 0.7735, mean 10-fold Precision = 0.802457540816 and mean 10-fold Recall = 0.827406078072\n",
    "\n",
    "Dirty Data: mean 10-fold Accuracy = 0.752, mean 10-fold Precision = 0.8067646146 and mean 10-fold Recall = 0.803254339078.\n",
    "\n",
    "Comparing the performaces of the decision tree modeled on the dirty data, we see that the accuracy and recall decreases by approximately by 2% and the precision increases by 0.04%, all which are not a very big difference. Since we add a lot of features in the dirty data, we increase the dimensionality which affect the model performance (the curse of dimensionality) and hence the model is more prone to overfit. Hence the slight differnce in the model with the dirty data. \n",
    "\n",
    "Analysis: We find the distribution of the violentCrime columns to all the columns that have missing value, The distribution table can be seen below. As we can see that the distribution is exactely the same for all the column with missing value, this cannot be just by chance. Hence we can say that the values are missing not at random.\n",
    "\n",
    "Also, the distribution is (violentCrimes=0.573731, nonViolent=crimes=0.426269)  which is not very far from the actual distibution of the label column which is (violentCrimes=62.72 and nonViolent-crimes=37.28). This is also a reason that we dont see a lot of change in the two decision tree model build with clean data and dirty data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 128)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>community</th>\n",
       "      <th>communityname</th>\n",
       "      <th>fold</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>numbUrban</th>\n",
       "      <th>pctUrban</th>\n",
       "      <th>medIncome</th>\n",
       "      <th>pctWWage</th>\n",
       "      <th>pctWFarmSelf</th>\n",
       "      <th>pctWInvInc</th>\n",
       "      <th>pctWSocSec</th>\n",
       "      <th>pctWPubAsst</th>\n",
       "      <th>pctWRetire</th>\n",
       "      <th>medFamInc</th>\n",
       "      <th>perCapInc</th>\n",
       "      <th>whitePerCap</th>\n",
       "      <th>blackPerCap</th>\n",
       "      <th>indianPerCap</th>\n",
       "      <th>AsianPerCap</th>\n",
       "      <th>OtherPerCap</th>\n",
       "      <th>HispPerCap</th>\n",
       "      <th>NumUnderPov</th>\n",
       "      <th>PctPopUnderPov</th>\n",
       "      <th>PctLess9thGrade</th>\n",
       "      <th>PctNotHSGrad</th>\n",
       "      <th>PctBSorMore</th>\n",
       "      <th>PctUnemployed</th>\n",
       "      <th>PctEmploy</th>\n",
       "      <th>PctEmplManu</th>\n",
       "      <th>PctEmplProfServ</th>\n",
       "      <th>PctOccupManu</th>\n",
       "      <th>PctOccupMgmtProf</th>\n",
       "      <th>MalePctDivorce</th>\n",
       "      <th>MalePctNevMarr</th>\n",
       "      <th>FemalePctDiv</th>\n",
       "      <th>TotalPctDiv</th>\n",
       "      <th>PersPerFam</th>\n",
       "      <th>PctFam2Par</th>\n",
       "      <th>PctKids2Par</th>\n",
       "      <th>PctYoungKids2Par</th>\n",
       "      <th>PctTeen2Par</th>\n",
       "      <th>PctWorkMomYoungKids</th>\n",
       "      <th>PctWorkMom</th>\n",
       "      <th>NumIlleg</th>\n",
       "      <th>PctIlleg</th>\n",
       "      <th>NumImmig</th>\n",
       "      <th>PctImmigRecent</th>\n",
       "      <th>PctImmigRec5</th>\n",
       "      <th>PctImmigRec8</th>\n",
       "      <th>PctImmigRec10</th>\n",
       "      <th>PctRecentImmig</th>\n",
       "      <th>PctRecImmig5</th>\n",
       "      <th>PctRecImmig8</th>\n",
       "      <th>PctRecImmig10</th>\n",
       "      <th>PctSpeakEnglOnly</th>\n",
       "      <th>PctNotSpeakEnglWell</th>\n",
       "      <th>PctLargHouseFam</th>\n",
       "      <th>PctLargHouseOccup</th>\n",
       "      <th>PersPerOccupHous</th>\n",
       "      <th>PersPerOwnOccHous</th>\n",
       "      <th>PersPerRentOccHous</th>\n",
       "      <th>PctPersOwnOccup</th>\n",
       "      <th>PctPersDenseHous</th>\n",
       "      <th>PctHousLess3BR</th>\n",
       "      <th>MedNumBR</th>\n",
       "      <th>HousVacant</th>\n",
       "      <th>PctHousOccup</th>\n",
       "      <th>PctHousOwnOcc</th>\n",
       "      <th>PctVacantBoarded</th>\n",
       "      <th>PctVacMore6Mos</th>\n",
       "      <th>MedYrHousBuilt</th>\n",
       "      <th>PctHousNoPhone</th>\n",
       "      <th>PctWOFullPlumb</th>\n",
       "      <th>OwnOccLowQuart</th>\n",
       "      <th>OwnOccMedVal</th>\n",
       "      <th>OwnOccHiQuart</th>\n",
       "      <th>RentLowQ</th>\n",
       "      <th>RentMedian</th>\n",
       "      <th>RentHighQ</th>\n",
       "      <th>MedRent</th>\n",
       "      <th>MedRentPctHousInc</th>\n",
       "      <th>MedOwnCostPctInc</th>\n",
       "      <th>MedOwnCostPctIncNoMtg</th>\n",
       "      <th>NumInShelters</th>\n",
       "      <th>NumStreet</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LemasSwornFT</th>\n",
       "      <th>LemasSwFTPerPop</th>\n",
       "      <th>LemasSwFTFieldOps</th>\n",
       "      <th>LemasSwFTFieldPerPop</th>\n",
       "      <th>LemasTotalReq</th>\n",
       "      <th>LemasTotReqPerPop</th>\n",
       "      <th>PolicReqPerOffic</th>\n",
       "      <th>PolicPerPop</th>\n",
       "      <th>RacialMatchCommPol</th>\n",
       "      <th>PctPolicWhite</th>\n",
       "      <th>PctPolicBlack</th>\n",
       "      <th>PctPolicHisp</th>\n",
       "      <th>PctPolicAsian</th>\n",
       "      <th>PctPolicMinor</th>\n",
       "      <th>OfficAssgnDrugUnits</th>\n",
       "      <th>NumKindsDrugsSeiz</th>\n",
       "      <th>PolicAveOTWorked</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Lakewoodcity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Tukwilacity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Aberdeentown</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>81440</td>\n",
       "      <td>Willingborotownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>95</td>\n",
       "      <td>6096</td>\n",
       "      <td>Bethlehemtownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.53</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state county community        communityname  fold  population  \\\n",
       "0      8      ?         ?         Lakewoodcity     1        0.19   \n",
       "1     53      ?         ?          Tukwilacity     1        0.00   \n",
       "2     24      ?         ?         Aberdeentown     1        0.00   \n",
       "3     34      5     81440  Willingborotownship     1        0.04   \n",
       "4     42     95      6096    Bethlehemtownship     1        0.01   \n",
       "\n",
       "   householdsize  racepctblack  racePctWhite  racePctAsian  racePctHisp  \\\n",
       "0           0.33          0.02          0.90          0.12         0.17   \n",
       "1           0.16          0.12          0.74          0.45         0.07   \n",
       "2           0.42          0.49          0.56          0.17         0.04   \n",
       "3           0.77          1.00          0.08          0.12         0.10   \n",
       "4           0.55          0.02          0.95          0.09         0.05   \n",
       "\n",
       "   agePct12t21  agePct12t29  agePct16t24  agePct65up  numbUrban  pctUrban  \\\n",
       "0         0.34         0.47         0.29        0.32       0.20       1.0   \n",
       "1         0.26         0.59         0.35        0.27       0.02       1.0   \n",
       "2         0.39         0.47         0.28        0.32       0.00       0.0   \n",
       "3         0.51         0.50         0.34        0.21       0.06       1.0   \n",
       "4         0.38         0.38         0.23        0.36       0.02       0.9   \n",
       "\n",
       "   medIncome  pctWWage  pctWFarmSelf  pctWInvInc  pctWSocSec  pctWPubAsst  \\\n",
       "0       0.37      0.72          0.34        0.60        0.29         0.15   \n",
       "1       0.31      0.72          0.11        0.45        0.25         0.29   \n",
       "2       0.30      0.58          0.19        0.39        0.38         0.40   \n",
       "3       0.58      0.89          0.21        0.43        0.36         0.20   \n",
       "4       0.50      0.72          0.16        0.68        0.44         0.11   \n",
       "\n",
       "   pctWRetire  medFamInc  perCapInc  whitePerCap  blackPerCap  indianPerCap  \\\n",
       "0        0.43       0.39       0.40         0.39         0.32          0.27   \n",
       "1        0.39       0.29       0.37         0.38         0.33          0.16   \n",
       "2        0.84       0.28       0.27         0.29         0.27          0.07   \n",
       "3        0.82       0.51       0.36         0.40         0.39          0.16   \n",
       "4        0.71       0.46       0.43         0.41         0.28          0.00   \n",
       "\n",
       "   AsianPerCap OtherPerCap  HispPerCap  NumUnderPov  PctPopUnderPov  \\\n",
       "0         0.27        0.36        0.41         0.08            0.19   \n",
       "1         0.30        0.22        0.35         0.01            0.24   \n",
       "2         0.29        0.28        0.39         0.01            0.27   \n",
       "3         0.25        0.36        0.44         0.01            0.10   \n",
       "4         0.74        0.51        0.48         0.00            0.06   \n",
       "\n",
       "   PctLess9thGrade  PctNotHSGrad  PctBSorMore  PctUnemployed  PctEmploy  \\\n",
       "0             0.10          0.18         0.48           0.27       0.68   \n",
       "1             0.14          0.24         0.30           0.27       0.73   \n",
       "2             0.27          0.43         0.19           0.36       0.58   \n",
       "3             0.09          0.25         0.31           0.33       0.71   \n",
       "4             0.25          0.30         0.33           0.12       0.65   \n",
       "\n",
       "   PctEmplManu  PctEmplProfServ  PctOccupManu  PctOccupMgmtProf  \\\n",
       "0         0.23             0.41          0.25              0.52   \n",
       "1         0.57             0.15          0.42              0.36   \n",
       "2         0.32             0.29          0.49              0.32   \n",
       "3         0.36             0.45          0.37              0.39   \n",
       "4         0.67             0.38          0.42              0.46   \n",
       "\n",
       "   MalePctDivorce  MalePctNevMarr  FemalePctDiv  TotalPctDiv  PersPerFam  \\\n",
       "0            0.68            0.40          0.75         0.75        0.35   \n",
       "1            1.00            0.63          0.91         1.00        0.29   \n",
       "2            0.63            0.41          0.71         0.70        0.45   \n",
       "3            0.34            0.45          0.49         0.44        0.75   \n",
       "4            0.22            0.27          0.20         0.21        0.51   \n",
       "\n",
       "   PctFam2Par  PctKids2Par  PctYoungKids2Par  PctTeen2Par  \\\n",
       "0        0.55         0.59              0.61         0.56   \n",
       "1        0.43         0.47              0.60         0.39   \n",
       "2        0.42         0.44              0.43         0.43   \n",
       "3        0.65         0.54              0.83         0.65   \n",
       "4        0.91         0.91              0.89         0.85   \n",
       "\n",
       "   PctWorkMomYoungKids  PctWorkMom  NumIlleg  PctIlleg  NumImmig  \\\n",
       "0                 0.74        0.76      0.04      0.14      0.03   \n",
       "1                 0.46        0.53      0.00      0.24      0.01   \n",
       "2                 0.71        0.67      0.01      0.46      0.00   \n",
       "3                 0.85        0.86      0.03      0.33      0.02   \n",
       "4                 0.40        0.60      0.00      0.06      0.00   \n",
       "\n",
       "   PctImmigRecent  PctImmigRec5  PctImmigRec8  PctImmigRec10  PctRecentImmig  \\\n",
       "0            0.24          0.27          0.37           0.39            0.07   \n",
       "1            0.52          0.62          0.64           0.63            0.25   \n",
       "2            0.07          0.06          0.15           0.19            0.02   \n",
       "3            0.11          0.20          0.30           0.31            0.05   \n",
       "4            0.03          0.07          0.20           0.27            0.01   \n",
       "\n",
       "   PctRecImmig5  PctRecImmig8  PctRecImmig10  PctSpeakEnglOnly  \\\n",
       "0          0.07          0.08           0.08              0.89   \n",
       "1          0.27          0.25           0.23              0.84   \n",
       "2          0.02          0.04           0.05              0.88   \n",
       "3          0.08          0.11           0.11              0.81   \n",
       "4          0.02          0.04           0.05              0.88   \n",
       "\n",
       "   PctNotSpeakEnglWell  PctLargHouseFam  PctLargHouseOccup  PersPerOccupHous  \\\n",
       "0                 0.06             0.14               0.13              0.33   \n",
       "1                 0.10             0.16               0.10              0.17   \n",
       "2                 0.04             0.20               0.20              0.46   \n",
       "3                 0.08             0.56               0.62              0.85   \n",
       "4                 0.05             0.16               0.19              0.59   \n",
       "\n",
       "   PersPerOwnOccHous  PersPerRentOccHous  PctPersOwnOccup  PctPersDenseHous  \\\n",
       "0               0.39                0.28             0.55              0.09   \n",
       "1               0.29                0.17             0.26              0.20   \n",
       "2               0.52                0.43             0.42              0.15   \n",
       "3               0.77                1.00             0.94              0.12   \n",
       "4               0.60                0.37             0.89              0.02   \n",
       "\n",
       "   PctHousLess3BR  MedNumBR  HousVacant  PctHousOccup  PctHousOwnOcc  \\\n",
       "0            0.51       0.5        0.21          0.71           0.52   \n",
       "1            0.82       0.0        0.02          0.79           0.24   \n",
       "2            0.51       0.5        0.01          0.86           0.41   \n",
       "3            0.01       0.5        0.01          0.97           0.96   \n",
       "4            0.19       0.5        0.01          0.89           0.87   \n",
       "\n",
       "   PctVacantBoarded  PctVacMore6Mos  MedYrHousBuilt  PctHousNoPhone  \\\n",
       "0              0.05            0.26            0.65            0.14   \n",
       "1              0.02            0.25            0.65            0.16   \n",
       "2              0.29            0.30            0.52            0.47   \n",
       "3              0.60            0.47            0.52            0.11   \n",
       "4              0.04            0.55            0.73            0.05   \n",
       "\n",
       "   PctWOFullPlumb  OwnOccLowQuart  OwnOccMedVal  OwnOccHiQuart  RentLowQ  \\\n",
       "0            0.06            0.22          0.19           0.18      0.36   \n",
       "1            0.00            0.21          0.20           0.21      0.42   \n",
       "2            0.45            0.18          0.17           0.16      0.27   \n",
       "3            0.11            0.24          0.21           0.19      0.75   \n",
       "4            0.14            0.31          0.31           0.30      0.40   \n",
       "\n",
       "   RentMedian  RentHighQ  MedRent  MedRentPctHousInc  MedOwnCostPctInc  \\\n",
       "0        0.35       0.38     0.34               0.38              0.46   \n",
       "1        0.38       0.40     0.37               0.29              0.32   \n",
       "2        0.29       0.27     0.31               0.48              0.39   \n",
       "3        0.70       0.77     0.89               0.63              0.51   \n",
       "4        0.36       0.38     0.38               0.22              0.51   \n",
       "\n",
       "   MedOwnCostPctIncNoMtg  NumInShelters  NumStreet  PctForeignBorn  \\\n",
       "0                   0.25           0.04        0.0            0.12   \n",
       "1                   0.18           0.00        0.0            0.21   \n",
       "2                   0.28           0.00        0.0            0.14   \n",
       "3                   0.47           0.00        0.0            0.19   \n",
       "4                   0.21           0.00        0.0            0.11   \n",
       "\n",
       "   PctBornSameState  PctSameHouse85  PctSameCity85  PctSameState85  \\\n",
       "0              0.42            0.50           0.51            0.64   \n",
       "1              0.50            0.34           0.60            0.52   \n",
       "2              0.49            0.54           0.67            0.56   \n",
       "3              0.30            0.73           0.64            0.65   \n",
       "4              0.72            0.64           0.61            0.53   \n",
       "\n",
       "  LemasSwornFT LemasSwFTPerPop LemasSwFTFieldOps LemasSwFTFieldPerPop  \\\n",
       "0         0.03            0.13              0.96                 0.17   \n",
       "1            ?               ?                 ?                    ?   \n",
       "2            ?               ?                 ?                    ?   \n",
       "3            ?               ?                 ?                    ?   \n",
       "4            ?               ?                 ?                    ?   \n",
       "\n",
       "  LemasTotalReq LemasTotReqPerPop PolicReqPerOffic PolicPerPop  \\\n",
       "0          0.06              0.18             0.44        0.13   \n",
       "1             ?                 ?                ?           ?   \n",
       "2             ?                 ?                ?           ?   \n",
       "3             ?                 ?                ?           ?   \n",
       "4             ?                 ?                ?           ?   \n",
       "\n",
       "  RacialMatchCommPol PctPolicWhite PctPolicBlack PctPolicHisp PctPolicAsian  \\\n",
       "0               0.94          0.93          0.03         0.07           0.1   \n",
       "1                  ?             ?             ?            ?             ?   \n",
       "2                  ?             ?             ?            ?             ?   \n",
       "3                  ?             ?             ?            ?             ?   \n",
       "4                  ?             ?             ?            ?             ?   \n",
       "\n",
       "  PctPolicMinor OfficAssgnDrugUnits NumKindsDrugsSeiz PolicAveOTWorked  \\\n",
       "0          0.07                0.02              0.57             0.29   \n",
       "1             ?                   ?                 ?                ?   \n",
       "2             ?                   ?                 ?                ?   \n",
       "3             ?                   ?                 ?                ?   \n",
       "4             ?                   ?                 ?                ?   \n",
       "\n",
       "   LandArea  PopDens  PctUsePubTrans PolicCars PolicOperBudg  \\\n",
       "0      0.12     0.26            0.20      0.06          0.04   \n",
       "1      0.02     0.12            0.45         ?             ?   \n",
       "2      0.01     0.21            0.02         ?             ?   \n",
       "3      0.02     0.39            0.28         ?             ?   \n",
       "4      0.04     0.09            0.02         ?             ?   \n",
       "\n",
       "  LemasPctPolicOnPatr LemasGangUnitDeploy  LemasPctOfficDrugUn  \\\n",
       "0                 0.9                 0.5                 0.32   \n",
       "1                   ?                   ?                 0.00   \n",
       "2                   ?                   ?                 0.00   \n",
       "3                   ?                   ?                 0.00   \n",
       "4                   ?                   ?                 0.00   \n",
       "\n",
       "  PolicBudgPerPop  ViolentCrimesPerPop  \n",
       "0            0.14                 0.20  \n",
       "1               ?                 0.67  \n",
       "2               ?                 0.43  \n",
       "3               ?                 0.12  \n",
       "4               ?                 0.03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datadir = '/Users/sam/All-Program/App-DataSet/Data-Science-Projects/Crime-Prediction/communities-crime-full.csv'\n",
    "\n",
    "crimeDF_dirty = pd.read_csv(datadir, header='infer', sep=',')\n",
    "\n",
    "print (crimeDF_dirty.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(crimeDF_dirty.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--> Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of dataForDT after preprocessing is:  (1994, 122)\n",
      "The shape of targetForDT after preprocessing is:  (1994,)\n",
      "[1 1 1 ..., 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>PolicAveOTWorked</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0        0.19           0.33          0.02          0.90          0.12   \n",
       "1        0.00           0.16          0.12          0.74          0.45   \n",
       "2        0.00           0.42          0.49          0.56          0.17   \n",
       "3        0.04           0.77          1.00          0.08          0.12   \n",
       "4        0.01           0.55          0.02          0.95          0.09   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  \\\n",
       "0         0.17         0.34         0.47         0.29        0.32   \n",
       "1         0.07         0.26         0.59         0.35        0.27   \n",
       "2         0.04         0.39         0.47         0.28        0.32   \n",
       "3         0.10         0.51         0.50         0.34        0.21   \n",
       "4         0.05         0.38         0.38         0.23        0.36   \n",
       "\n",
       "        ...         PolicAveOTWorked  LandArea  PopDens  PctUsePubTrans  \\\n",
       "0       ...                     0.29      0.12     0.26            0.20   \n",
       "1       ...                        ?      0.02     0.12            0.45   \n",
       "2       ...                        ?      0.01     0.21            0.02   \n",
       "3       ...                        ?      0.02     0.39            0.28   \n",
       "4       ...                        ?      0.04     0.09            0.02   \n",
       "\n",
       "   PolicCars  PolicOperBudg  LemasPctPolicOnPatr  LemasGangUnitDeploy  \\\n",
       "0       0.06           0.04                  0.9                  0.5   \n",
       "1          ?              ?                    ?                    ?   \n",
       "2          ?              ?                    ?                    ?   \n",
       "3          ?              ?                    ?                    ?   \n",
       "4          ?              ?                    ?                    ?   \n",
       "\n",
       "   LemasPctOfficDrugUn  PolicBudgPerPop  \n",
       "0                 0.32             0.14  \n",
       "1                 0.00                ?  \n",
       "2                 0.00                ?  \n",
       "3                 0.00                ?  \n",
       "4                 0.00                ?  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonPredictiveFeatures = ['state','county','community', 'communityname','fold']\n",
    "regTargetVar = ['ViolentCrimesPerPop']\n",
    "classTargetVar = ['highCrime']\n",
    "\n",
    "# Convert the lebel form continuous to discrete\n",
    "crimeDF_dirty['highCrime'] = crimeDF_dirty['ViolentCrimesPerPop'] > 0.1\n",
    "\n",
    "crimeDF_dirty.head()\n",
    "\n",
    "# converting the label into proper binary labels\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# # Remove the non-predictive and the target column form the data\n",
    "dataContinuous_dirty = crimeDF_dirty.drop(nonPredictiveFeatures+regTargetVar+classTargetVar, 1)  \n",
    "# # Convert the target column into binomial label vector\n",
    "targetBinomial_dirty = lb.fit_transform(crimeDF_dirty['highCrime']).flatten()      \n",
    "\n",
    "print ('The shape of dataForDT after preprocessing is: ', dataContinuous_dirty.shape)\n",
    "print ('The shape of targetForDT after preprocessing is: ', targetBinomial_dirty.shape)\n",
    "# print (dataForDT)\n",
    "print (targetBinomial_dirty)\n",
    "dataContinuous_dirty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--> Imputing missing values with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>PolicAveOTWorked</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305987</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.163103</td>\n",
       "      <td>0.076708</td>\n",
       "      <td>0.698589</td>\n",
       "      <td>0.440439</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.195078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305987</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.163103</td>\n",
       "      <td>0.076708</td>\n",
       "      <td>0.698589</td>\n",
       "      <td>0.440439</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.195078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305987</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.163103</td>\n",
       "      <td>0.076708</td>\n",
       "      <td>0.698589</td>\n",
       "      <td>0.440439</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.195078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305987</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.163103</td>\n",
       "      <td>0.076708</td>\n",
       "      <td>0.698589</td>\n",
       "      <td>0.440439</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.195078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0        0.19           0.33          0.02          0.90          0.12   \n",
       "1        0.00           0.16          0.12          0.74          0.45   \n",
       "2        0.00           0.42          0.49          0.56          0.17   \n",
       "3        0.04           0.77          1.00          0.08          0.12   \n",
       "4        0.01           0.55          0.02          0.95          0.09   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  \\\n",
       "0         0.17         0.34         0.47         0.29        0.32   \n",
       "1         0.07         0.26         0.59         0.35        0.27   \n",
       "2         0.04         0.39         0.47         0.28        0.32   \n",
       "3         0.10         0.51         0.50         0.34        0.21   \n",
       "4         0.05         0.38         0.38         0.23        0.36   \n",
       "\n",
       "        ...         PolicAveOTWorked  LandArea  PopDens  PctUsePubTrans  \\\n",
       "0       ...                 0.290000      0.12     0.26            0.20   \n",
       "1       ...                 0.305987      0.02     0.12            0.45   \n",
       "2       ...                 0.305987      0.01     0.21            0.02   \n",
       "3       ...                 0.305987      0.02     0.39            0.28   \n",
       "4       ...                 0.305987      0.04     0.09            0.02   \n",
       "\n",
       "   PolicCars  PolicOperBudg  LemasPctPolicOnPatr  LemasGangUnitDeploy  \\\n",
       "0   0.060000       0.040000             0.900000             0.500000   \n",
       "1   0.163103       0.076708             0.698589             0.440439   \n",
       "2   0.163103       0.076708             0.698589             0.440439   \n",
       "3   0.163103       0.076708             0.698589             0.440439   \n",
       "4   0.163103       0.076708             0.698589             0.440439   \n",
       "\n",
       "   LemasPctOfficDrugUn  PolicBudgPerPop  \n",
       "0                 0.32         0.140000  \n",
       "1                 0.00         0.195078  \n",
       "2                 0.00         0.195078  \n",
       "3                 0.00         0.195078  \n",
       "4                 0.00         0.195078  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We inpute missing value with the mean because there is a \n",
    "# high probability of data having a value of 0 \n",
    "# so imputing with zero would bais the decision tree model\n",
    "# Also, there is very less probability for the data\n",
    "# to have the value of exact mean at float64.\n",
    "# Also, For decision tree classification, since we are splitting,\n",
    "# imputing any value should not make a lot of difference\n",
    "\n",
    "dataContinuous_dirtyMean = dataContinuous_dirty\n",
    "\n",
    "# First We replace all the ? with np.NaN\n",
    "dataContinuous_dirtyMean = dataContinuous_dirtyMean.replace(to_replace='?', value=np.NaN)\n",
    "dataContinuous_dirtyMean = dataContinuous_dirtyMean.astype('float64')\n",
    "\n",
    "meanDF = dataContinuous_dirtyMean.mean(axis=0, skipna=True)\n",
    "\n",
    "def inputeMean(dataFrameIN):\n",
    "    for columns in dataFrameIN.columns:\n",
    "        dataFrameIN[columns] = dataFrameIN[columns].replace(to_replace=np.NaN, value=meanDF[columns])\n",
    "    return (dataFrameIN)\n",
    "    \n",
    "dataContinuous_dirtyMean = inputeMean(dataContinuous_dirtyMean)\n",
    "\n",
    "dataContinuous_dirtyMean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--> Fit the Decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.75600000000000001},\n",
       " {'precision': 0.8109243885090367},\n",
       " {'recall': 0.80587961948526821}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the Decision Tree Model\n",
    "classifierDT_dirty = tree.DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "modelName='DirtyData: Decision Tree Classifier'\n",
    "scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierDT_dirty)\n",
    "scoreOBJ.setData(dataIN=dataContinuous_dirtyMean, targetIN=targetBinomial_dirty)\n",
    "scoreOBJ.cvEvaluate(scoringMetric=['accuracy','precision','recall'])\n",
    "\n",
    "modelScores[modelName]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--> Find the distribution of the class variable given few column with missing value (the column data in grouped by on missing value \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%non-ViolentCrime</th>\n",
       "      <th>%ViolentCrime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <td>0.426269</td>\n",
       "      <td>0.573731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <td>0.426269</td>\n",
       "      <td>0.573731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <td>0.426269</td>\n",
       "      <td>0.573731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PolicCars</th>\n",
       "      <td>0.426269</td>\n",
       "      <td>0.573731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PolicAveOTWorked</th>\n",
       "      <td>0.426269</td>\n",
       "      <td>0.573731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "      <td>0.426269</td>\n",
       "      <td>0.573731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    %non-ViolentCrime %ViolentCrime\n",
       "LemasPctPolicOnPatr          0.426269      0.573731\n",
       "LemasGangUnitDeploy          0.426269      0.573731\n",
       "PolicOperBudg                0.426269      0.573731\n",
       "PolicCars                    0.426269      0.573731\n",
       "PolicAveOTWorked             0.426269      0.573731\n",
       "PolicBudgPerPop              0.426269      0.573731"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def checkMissingDist(dataFrameIN, columnNameList):\n",
    "    distMissing_DF = pd.DataFrame(columns=['%non-ViolentCrime', '%ViolentCrime'])\n",
    "    for columnName in columnNameList:\n",
    "        dfNew = pd.DataFrame(columns=[columnName, 'target'])\n",
    "        dfNew[columnName] = dataFrameIN[columnName]\n",
    "        dfNew['target'] = targetBinomial_dirty\n",
    "        dfcrosstab = dfNew[(dfNew[columnName] == '?')].groupby('target').size()\n",
    "#         print (dfcrosstab[0]/(dfcrosstab[0]+dfcrosstab[1]))\n",
    "#         print (dfcrosstab[1]/(dfcrosstab[0]+dfcrosstab[1]))\n",
    "        distMissing_DF.ix[columnName,'%non-ViolentCrime'] = dfcrosstab[0]/(dfcrosstab[0]+dfcrosstab[1])\n",
    "        distMissing_DF.ix[columnName,'%ViolentCrime'] = dfcrosstab[1]/(dfcrosstab[0]+dfcrosstab[1])\n",
    "    return distMissing_DF\n",
    "    \n",
    "columnNameList = ['LemasPctPolicOnPatr', 'LemasGangUnitDeploy', 'PolicOperBudg', 'PolicCars', 'PolicAveOTWorked', 'PolicBudgPerPop']\n",
    "distMissing_DF = checkMissingDist(dataContinuous_dirty, columnNameList)\n",
    "                                  \n",
    "distMissing_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--> Imputing missing values with zero (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dataContinuous_dirty0 = dataContinuous_dirty\n",
    "\n",
    "# # We replace all the ? with 0\n",
    "# dataContinuous_dirty0 = dataContinuous_dirty0.replace(to_replace='?', value=0)\n",
    "\n",
    "# # We convert he datatypes into float64\n",
    "# dataContinuous_dirty0 = dataContinuous_dirty0.astype('float64')\n",
    "\n",
    "# # Decision Tree Classifier\n",
    "# classifierDT_dirty = tree.DecisionTreeClassifier()\n",
    "\n",
    "# # Performance Metric\n",
    "# nfoldACC = model_selection.cross_val_score(classifierDT_dirty, dataContinuous_dirty0, targetBinomial_dirty, cv=cv, scoring='accuracy')\n",
    "# nfoldPrecision = model_selection.cross_val_score(classifierDT_dirty, dataContinuous_dirty0, targetBinomial_dirty, cv=cv, scoring='precision')\n",
    "# nfoldRecall = model_selection.cross_val_score(classifierDT_dirty, dataContinuous_dirty0, targetBinomial_dirty, cv=cv, scoring='recall')\n",
    "\n",
    "# print ('The 10 Fold accuracy of the decision tree classifier is %s and their mean is : '%(str(nfoldACC)), np.mean(nfoldACC))\n",
    "# print ('The 10 Fold precision of the decision tree classifier is %s and their mean is : '%(str(nfoldPrecision)), np.mean(nfoldPrecision))\n",
    "# print ('The 10 Fold recall of the decision tree classifier is %s and their mean is : '%(str(nfoldRecall)), np.mean(nfoldRecall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Extra Credits\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part (a) :- Do a team requirement (above) that your team is not already required to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Step:\n",
    "\n",
    "Step 1: We replace all the missing values of the dirty data with the mean. We also experiment with median, but the feature selection technique doesnt choose any of the features with missing data.\n",
    "\n",
    "Step 2: We use Lasso Feature selection technique to find the most predictive features in the dataset (both clean and dirty). Lasso finds 34 most important features out of the 100 features from the clean dataset and 37 features out of 122 features from the Dirty Data.\n",
    "\n",
    "Step 3: To the new data (clean and dirty) consisting only the features selected, we fit a simple Logistic Regression model and compute the precision, recall and accuracy using CrossValidation.\n",
    "\n",
    "Step 4: To the new data (clean and dirty) consisting only the features selected, we fit a SVM model usign a polynomial kernel (3rd order) and compute the precision, recall and accuracy using CrossValidation.\n",
    "\n",
    "Note: Lasso Regression and Logistic Regression with L1 norm produces different outputs and performance mesure. While Evaluating models we see that the features selection with Lasso using Logistic (with L1 norm) was better than simple Logistic with L1 norm. That is the reason we do LASSO feature selection separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1) Experiment with two learning methods other than those described above (one can be a non-linear kernel for SVM) for the classification problem, explaining clearly what you did. Show CV results for both the clean and full datasets.\n",
    "\n",
    "Cross Validation Results:\n",
    "\n",
    "Clean Data; Model (Lasso Feature selection; Logistic with L1 Norm) Performance Metric: [{'accuracy': 0.85149999999999992}, {'precision': 0.8709122026050311}, {'recall': 0.88880138359548133}]\n",
    "\n",
    "Clean Data: Model (Lasso Feature selection; Poly SVM) : Performance Metric: [{'accuracy': 0.83050000000000002}, {'precision': 0.84073677346707765}, {'recall': 0.89131720925893521}]\n",
    "\n",
    "Dirty Data: Model (Lasso Feature selection; Logistic with L1 Norm) : Performance Metric: [{'accuracy': 0.84999999999999998}, {'precision': 0.88315316441279157}, {'recall': 0.88196204423858404}]\n",
    "\n",
    "Dirty Data: Model (Lasso Feature selection; Poly SVM) : Performance Metric: [{'accuracy': 0.83699999999999997}, {'precision': 0.86248894734647141}, {'recall': 0.88574373106459969}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ii) What method gives the best results?\n",
    "\n",
    "Solution: Logistic regression on feature selection with LASSO gives the best Result. Because the cross validation accuracy, precision and recall are  better than the polynomial SVM of 3rd degree. Additionally, the model size in the simple Logistic regression is only 34 which is very very small compared to the 3rd order polynomial feature space of SVM. This satisfies Ocam Razor that states that the smaller model that provides better performance cannot be by chance. \n",
    "\n",
    "The evaluation score of models with dirty data is very close to the evaluation score of models with clean data. This is because the LASSO feature selection technique removes all the irrelevant featues from the dirty data. This also states that the features with missing values in the dirty data are not relevant and does not improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### iii) What feature(s) seem to be most consistently predictive of high crime rates? How reliable is this conclusion?\n",
    "\n",
    "TotalPctDiv: Has +ve predictive coefficient for classification probelems (Gaussian Naive bayes) but negative cofficient for regression problems.\n",
    "\n",
    "racePctHisp: Was said to have high information gain for decision trees\n",
    "\n",
    "population was also said to be a +ve predictive feature for Linear SVC\n",
    "\n",
    "RentHighQ was said to have +ve predictive power for linear SVC\n",
    "\n",
    "FemalePctDiv was said to have +ve predictive power for Gaisian Naive Bayes\n",
    "\n",
    "PctTeen2Par +ve predictive power for gaussian naive bayes\n",
    "\n",
    "The conclusion are reliable because the model are evaluated using 10 fold cross validation. The model (Lasso features with Logistic) produces the best results when compared to all other models only by using 34 features. Most of the features indicating low crimes and some of the features high crimes that are selected were also selected by other models such as Linear SVC, Gaussian naive bayes and others. This also states that the featues selection is proper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--> Clean Data: Lasso Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Clean Data is:  (1993, 100)\n",
      "CleanData: Number of Features selected by Lasso is:  34\n",
      "The first degree features most predictive of a high crime rate and low crime rate and their respective weights are: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highCrimePredictive_features</th>\n",
       "      <th>high_featureWeight</th>\n",
       "      <th>lowCrimePredictive_features</th>\n",
       "      <th>low_featureWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TotalPctDiv</td>\n",
       "      <td>0.351936</td>\n",
       "      <td>pctWInvInc</td>\n",
       "      <td>-0.765809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>racePctHisp</td>\n",
       "      <td>0.217541</td>\n",
       "      <td>PctKids2Par</td>\n",
       "      <td>-0.412368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OwnOccHiQuart</td>\n",
       "      <td>0.114368</td>\n",
       "      <td>PersPerOwnOccHous</td>\n",
       "      <td>-0.242417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OtherPerCap</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>racePctWhite</td>\n",
       "      <td>-0.227684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>population</td>\n",
       "      <td>0.084624</td>\n",
       "      <td>NumIlleg</td>\n",
       "      <td>-0.152096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  highCrimePredictive_features  high_featureWeight  \\\n",
       "0                  TotalPctDiv            0.351936   \n",
       "1                  racePctHisp            0.217541   \n",
       "2                OwnOccHiQuart            0.114368   \n",
       "3                  OtherPerCap            0.099266   \n",
       "4                   population            0.084624   \n",
       "\n",
       "  lowCrimePredictive_features  low_featureWeight  \n",
       "0                  pctWInvInc          -0.765809  \n",
       "1                 PctKids2Par          -0.412368  \n",
       "2           PersPerOwnOccHous          -0.242417  \n",
       "3                racePctWhite          -0.227684  \n",
       "4                    NumIlleg          -0.152096  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "\n",
    "print ('The shape of Clean Data is: ', dataContinuous.shape)\n",
    "\n",
    "# Fit LASSO (Linear regression with L1 Norm)\n",
    "classifierLS_clean = linear_model.Lasso(alpha=0.001, \n",
    "                                  fit_intercept=True, \n",
    "                                  max_iter=1000)\n",
    "\n",
    "classifierLS_clean.fit(dataContinuous, targetBinomial)\n",
    "\n",
    "print ('CleanData: Number of Features selected by Lasso is: ', len(np.where(classifierLS_clean.coef_ != 0)[0]))\n",
    "\n",
    "\n",
    "\n",
    "modelName='Clean Data: Lasso Features'\n",
    "clnLasso_scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierLS_clean)\n",
    "clnLasso_scoreOBJ.setData(dataIN=dataContinuous, targetIN=targetBinomial)\n",
    "topFeatureDF = clnLasso_scoreOBJ.getTopFeatures(num=5)\n",
    "print ('The first degree features most predictive of a high crime rate and low crime rate and their respective weights are: ')\n",
    "topFeatureDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--> Dirty Data: Lasso Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Clean Data is:  (1994, 122)\n",
      "CleanData: Number of Features selected by Lasso is:  37\n",
      "The first degree features most predictive of a high crime rate and low crime rate and their respective weights are: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highCrimePredictive_features</th>\n",
       "      <th>high_featureWeight</th>\n",
       "      <th>lowCrimePredictive_features</th>\n",
       "      <th>low_featureWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TotalPctDiv</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>pctWInvInc</td>\n",
       "      <td>-0.758303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RacialMatchCommPol</td>\n",
       "      <td>0.288589</td>\n",
       "      <td>PctKids2Par</td>\n",
       "      <td>-0.405172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>racePctHisp</td>\n",
       "      <td>0.201255</td>\n",
       "      <td>racePctWhite</td>\n",
       "      <td>-0.287850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OwnOccHiQuart</td>\n",
       "      <td>0.118270</td>\n",
       "      <td>PersPerOwnOccHous</td>\n",
       "      <td>-0.217569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OtherPerCap</td>\n",
       "      <td>0.101677</td>\n",
       "      <td>PctPolicMinor</td>\n",
       "      <td>-0.183767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  highCrimePredictive_features  high_featureWeight  \\\n",
       "0                  TotalPctDiv            0.374450   \n",
       "1           RacialMatchCommPol            0.288589   \n",
       "2                  racePctHisp            0.201255   \n",
       "3                OwnOccHiQuart            0.118270   \n",
       "4                  OtherPerCap            0.101677   \n",
       "\n",
       "  lowCrimePredictive_features  low_featureWeight  \n",
       "0                  pctWInvInc          -0.758303  \n",
       "1                 PctKids2Par          -0.405172  \n",
       "2                racePctWhite          -0.287850  \n",
       "3           PersPerOwnOccHous          -0.217569  \n",
       "4               PctPolicMinor          -0.183767  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('The shape of Clean Data is: ', dataContinuous_dirtyMean.shape)\n",
    "\n",
    "classifierLS_dirty = linear_model.Lasso(alpha=0.001, \n",
    "                                  fit_intercept=True, \n",
    "                                  max_iter=1000)\n",
    "\n",
    "classifierLS_dirty.fit(dataContinuous_dirtyMean, targetBinomial_dirty)\n",
    "\n",
    "print ('CleanData: Number of Features selected by Lasso is: ', len(np.where(classifierLS_dirty.coef_ != 0)[0]))\n",
    "\n",
    "\n",
    "\n",
    "modelName='Dirty Data: Lasso Features'\n",
    "drtyLasso_scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierLS_dirty)\n",
    "drtyLasso_scoreOBJ.setData(dataIN=dataContinuous_dirtyMean, targetIN=targetBinomial_dirty)\n",
    "topFeatureDF = drtyLasso_scoreOBJ.getTopFeatures(num=5)\n",
    "print ('The first degree features most predictive of a high crime rate and low crime rate and their respective weights are: ')\n",
    "topFeatureDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--> Logistic Regresion with L1 norm: Clean Data and Dirty Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Data: The Shape of New data is:  (1993, 34)\n",
      "Model (Clean Data; Lasso Feature selection; Logistic with L1 Norm) : Performance Metric: \n",
      " [{'accuracy': 0.85149999999999992}, {'precision': 0.8709122026050311}, {'recall': 0.88880138359548133}]\n",
      "\n",
      "Dirty Data: The Shape of New data is:  (1994, 37)\n",
      "Model (Dirty Data; Lasso Feature selection; Logistic with L1 Norm) : Performance Metric: \n",
      " [{'accuracy': 0.84999999999999998}, {'precision': 0.88315316441279157}, {'recall': 0.88196204423858404}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Logistic model with L1 norm:\n",
    "C = 1\n",
    "classifierLR_L1_poly = linear_model.LogisticRegression(penalty='l1', \n",
    "                                                       C=C, fit_intercept=True, \n",
    "                                                       max_iter=1000)\n",
    "\n",
    "####### For Clean Data\n",
    "newCoeff = classifierLS_clean.coef_.flatten()\n",
    "newCleanData = dataContinuous[dataContinuous.columns[np.where(newCoeff!=0)[0]]]\n",
    "print ('Clean Data: The Shape of New data is: ', newCleanData.shape)\n",
    "\n",
    "modelName='Clean Data; Lasso Feature selection; Logistic with L1 Norm'\n",
    "cleanLasso_scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierLR_L1_poly)\n",
    "cleanLasso_scoreOBJ.setData(dataIN=newCleanData, targetIN=targetBinomial)\n",
    "cleanLasso_scoreOBJ.cvEvaluate(scoringMetric=['accuracy','precision','recall'])\n",
    "\n",
    "print ('Model (%s) : Performance Metric: \\n'%modelName, modelScores[modelName])\n",
    "print ('')\n",
    "\n",
    "\n",
    "\n",
    "#######  For Dirty Data\n",
    "newCoeff = classifierLS_dirty.coef_.flatten()\n",
    "newDirtyData = dataContinuous_dirtyMean[dataContinuous_dirtyMean.columns[np.where(newCoeff!=0)[0]]]\n",
    "print ('Dirty Data: The Shape of New data is: ', newDirtyData.shape)\n",
    "\n",
    "modelName='Dirty Data; Lasso Feature selection; Logistic with L1 Norm'\n",
    "dirtyLasso_scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierLR_L1_poly)\n",
    "dirtyLasso_scoreOBJ.setData(dataIN=newDirtyData, targetIN=targetBinomial_dirty)\n",
    "dirtyLasso_scoreOBJ.cvEvaluate(scoringMetric=['accuracy','precision','recall'])\n",
    "\n",
    "print ('Model (%s) : Performance Metric: \\n'%modelName, modelScores[modelName])\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--> Poly Kernel SVM: Clean Data and Dirty Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Data: The Shape of New data is:  (1993, 34)\n",
      "Model (Clean Data; Lasso Feature selection; Poly SVM) : Performance Metric: \n",
      " [{'accuracy': 0.83050000000000002}, {'precision': 0.84073677346707765}, {'recall': 0.89131720925893521}]\n",
      "\n",
      "Dirty Data: The Shape of New data is:  (1994, 37)\n",
      "Model (Dirty Data; Lasso Feature selection; Poly SVM) : Performance Metric: \n",
      " [{'accuracy': 0.83699999999999997}, {'precision': 0.86248894734647141}, {'recall': 0.88574373106459969}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifierSVM_poly = svm.SVC(kernel='poly', gamma=0.1, C=0.5, degree=3)\n",
    "\n",
    "\n",
    "####### For Clean Data\n",
    "newCoeff = classifierLS_clean.coef_.flatten()\n",
    "newCleanData = dataContinuous[dataContinuous.columns[np.where(newCoeff!=0)[0]]]\n",
    "print ('Clean Data: The Shape of New data is: ', newCleanData.shape)\n",
    "\n",
    "modelName='Clean Data; Lasso Feature selection; Poly SVM'\n",
    "cleanLasso_scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierSVM_poly)\n",
    "cleanLasso_scoreOBJ.setData(dataIN=newCleanData, targetIN=targetBinomial)\n",
    "cleanLasso_scoreOBJ.cvEvaluate(scoringMetric=['accuracy','precision','recall'])\n",
    "\n",
    "print ('Model (%s) : Performance Metric: \\n'%modelName, modelScores[modelName])\n",
    "print ('')\n",
    "\n",
    "\n",
    "\n",
    "#######  For Dirty Data\n",
    "newCoeff = classifierLS_dirty.coef_.flatten()\n",
    "newDirtyData = dataContinuous_dirtyMean[dataContinuous_dirtyMean.columns[np.where(newCoeff!=0)[0]]]\n",
    "print ('Dirty Data: The Shape of New data is: ', newDirtyData.shape)\n",
    "\n",
    "modelName='Dirty Data; Lasso Feature selection; Poly SVM'\n",
    "dirtyLasso_scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierSVM_poly)\n",
    "dirtyLasso_scoreOBJ.setData(dataIN=newDirtyData, targetIN=targetBinomial_dirty)\n",
    "dirtyLasso_scoreOBJ.cvEvaluate(scoringMetric=['accuracy','precision','recall'])\n",
    "\n",
    "print ('Model (%s) : Performance Metric: \\n'%modelName, modelScores[modelName])\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part (b) :- Experiment with other learning methods such as polynomial or other kernels in SVM, decision forests, boosting, etc. and show your results. Make sure to explain clearly what you did.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Steps:\n",
    "\n",
    "Here we build and evaluate three different models. 1) SVM with rbf Kernel, 2) Random Forest and 3) Adaboost. \n",
    "\n",
    "Step 1: First we define a range of coefficients for each parameter c and gamma for the rbf kernel. We run the SVM model for all different combination of c and gamma and then find the accuracy, precision and recall using a 10 fold cross validation\n",
    "\n",
    "Step 2: Second we fit a random forest model for 500 decision trees for 10 fold cross validation and evaluate the model.\n",
    "\n",
    "Step 3: Third, we fit a adaboost model with for 500 estimators for 10 fold cross validation and eveluate the models\n",
    "\n",
    "\n",
    "Approach: Here we define a range of coefficients for each parameter \"c\" and \"gamma\", Using a grid like structure we fit a \"rbf\" kernel for 10-fold cross validation. \"c\" makes the trade of between the misclassification error and simpler model. A higher \"c\" would try to learn the training data and find more support vectors to support a very high degree of polynomial curve to fit all training points (non-regularized). \"gamma\" can be thought of as how much inverse of variance are we willing to trade of for the support vectors. A high value of \"gamma\" would result in the radius of the area of influence of the support vectors only including the support vector.\n",
    "\n",
    "From the result we see that the best result is at [c= 1.0 and gamma = 0.01]\t[acc=0.818322, precision=0.900897, recall=0.8624]. This indicates a small value of \"gamma\" which can be understood that the dataset doesn't have a very good decision split. The smaller \"gamma\" also indicates that the model is unable to find the complex structure in the data (even though it does, the model doesn't generalize well for the validation dataset), hence we can think of the model as close to a linear fit. Thats the rason that the evaluation metric of the \"rbf\" kernel is very close to \"linear SVC\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### i) What method gives the best results?\n",
    "\n",
    "Solution: From the below evaluation score we can see that overall Random forest performs better than SVM (rbf) and adaboost. The Random forest in an ensemble technique that average the output from many decision tree for random samples.\n",
    "\n",
    "SVM RBF: At c=1.0 and gamma=0.01: Performance Metric: {accuracy:0.818322,  precision:0.900897,  recall:0.8624}\n",
    "\n",
    "Random Forest: num_trees=500 {'accuracy': 0.84199999999999997}, {'precision': 0.86346677944514327}, {'recall': 0.88492710688727683}\n",
    "\n",
    "AdaBoost: estimators=500, {'accuracy': 0.81649999999999989}, {'precision': 0.84281571421416412}, {'recall': 0.85999477916895373} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ii) What feature(s) seem to be most consistently predictive of high crime rates? How reliable is this conclusion?\n",
    "\n",
    "Solution: From the best features we can see that features such as {PctKids2Par, TotalPctDiv, racePctWhite, PctFam2Par} are choosen by both the adaboost and randomforest model as important features. Additionally\n",
    "\n",
    "SVM - RBF kernel: The solution is reliable in a sense that the dynamics of SVM is robust to overfitting. For classifying a new data point, despite the infinite dimensionality of the rbf kernel, we only have to go through support vectors, which are very less in number. The same has been processed through the crossvalidation. Moreover, The precision and recall scores are very high meaning that the algorithm is doing very good at identifying vilont crimes. Moreover cmpared to some of the above approaches the accuracy is also good enough.\n",
    "\n",
    "Random Forest: Random Forest are very robust to overfitting as they fit many decision tree for randomly sampled data and average the knowledge of all the trees. Random forest were seen to be second best model just after logistic regression with Lasso feature selection. The performance of the random forest model is also consistent making is reliable for the dataset.\n",
    "\n",
    "AdaBoost: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--> SVM with RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "class SVM_model():\n",
    "\n",
    "    def __init__(self, kernel='rbf'):\n",
    "#         self.c_range = [0.01, 0.1, 1.0, 10.0, 100.0]#, 1000.0]            \n",
    "#         self.gamma_range = [0.001, 0.01, 0.1, 1, 10.0, 100]  \n",
    "        self.c_range = [0.01, 0.05, 0.1, 0.5, 1] #, 1000.0]            \n",
    "        self.gamma_range = [0.01, 0.05, 0.1, 0.5, 1] \n",
    "        \n",
    "        self.kernel=kernel\n",
    "\n",
    "    def classify(self, trainData, trainLabels, validData):\n",
    "        c_range = self.c_range\n",
    "        gamma_range = self.gamma_range\n",
    "        \n",
    "        pred_dict = {}\n",
    "        for (c,gamma) in itertools.product(c_range, gamma_range):\n",
    "            string = \"c\" + str(c) + \"_\" + \"gamma\" + str(gamma)  \n",
    "            clf = svm.SVC(kernel=self.kernel, C=c, gamma=gamma)\n",
    "            classifier = clf.fit(trainData,trainLabels)\n",
    "            pred_dict[string] = clf.predict(validData)\n",
    "        return pred_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Since the classes \"HighCrime\", \"LowCrime\" are unbalanced, We do stratifies Sampling and fetch 10-fold cross validation dataset.\n",
    "import itertools\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def buildModel(data, target):\n",
    "    dataContinuous_NDarr = np.array(data, dtype='float64')\n",
    "    targetBinomial_NDarr = np.array(target, dtype='float64')\n",
    "\n",
    "    stN_Fold = StratifiedKFold(n_splits=10, random_state=675)  # We use the same seed, to get a rought comparison of the model with all other models.\n",
    "    stN_Fold.get_n_splits(dataContinuous_NDarr, targetBinomial_NDarr)\n",
    "#     print (stN_Fold)\n",
    "\n",
    "\n",
    "    objSVM = SVM_model(kernel='rbf')\n",
    "    outputEvaluationDict = {}\n",
    "    validLabelDict = {}\n",
    "    for foldNUM, (trainIndex, validIndex) in enumerate(stN_Fold.split(dataContinuous_NDarr, targetBinomial_NDarr)):\n",
    "#         print(\"Train:\", len(trainIndex), \"CrossValid:\", len(validIndex))\n",
    "        if len(trainIndex)+len(validIndex) != len(dataContinuous):\n",
    "            raise ValueError('The length of the samples doesnt match')\n",
    "\n",
    "\n",
    "        outputEvaluationDict[foldNUM] = objSVM.classify(trainData=dataContinuous_NDarr[trainIndex], \n",
    "                                                        trainLabels=targetBinomial_NDarr[trainIndex], \n",
    "                                                        validData=dataContinuous_NDarr[validIndex])\n",
    "        validLabelDict[foldNUM] = targetBinomial_NDarr[validIndex]\n",
    "    return outputEvaluationDict, validLabelDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg-accuracy</th>\n",
       "      <th>avg-precision</th>\n",
       "      <th>avg-recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c0.01_gamma0.01</th>\n",
       "      <td>0.627198</td>\n",
       "      <td>0.690013</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.01_gamma0.05</th>\n",
       "      <td>0.627198</td>\n",
       "      <td>0.690013</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.01_gamma0.1</th>\n",
       "      <td>0.654769</td>\n",
       "      <td>0.718544</td>\n",
       "      <td>0.9960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.01_gamma0.5</th>\n",
       "      <td>0.627198</td>\n",
       "      <td>0.690013</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.01_gamma1</th>\n",
       "      <td>0.627198</td>\n",
       "      <td>0.690013</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.05_gamma0.01</th>\n",
       "      <td>0.648756</td>\n",
       "      <td>0.712208</td>\n",
       "      <td>0.9976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.05_gamma0.05</th>\n",
       "      <td>0.789216</td>\n",
       "      <td>0.865033</td>\n",
       "      <td>0.8760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.05_gamma0.1</th>\n",
       "      <td>0.788206</td>\n",
       "      <td>0.864716</td>\n",
       "      <td>0.8680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.05_gamma0.5</th>\n",
       "      <td>0.749573</td>\n",
       "      <td>0.821524</td>\n",
       "      <td>0.9104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.05_gamma1</th>\n",
       "      <td>0.656796</td>\n",
       "      <td>0.722586</td>\n",
       "      <td>0.9776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.1_gamma0.01</th>\n",
       "      <td>0.766136</td>\n",
       "      <td>0.836136</td>\n",
       "      <td>0.9112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.1_gamma0.05</th>\n",
       "      <td>0.802256</td>\n",
       "      <td>0.880729</td>\n",
       "      <td>0.8688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.1_gamma0.1</th>\n",
       "      <td>0.806776</td>\n",
       "      <td>0.885797</td>\n",
       "      <td>0.8696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.1_gamma0.5</th>\n",
       "      <td>0.776168</td>\n",
       "      <td>0.851007</td>\n",
       "      <td>0.8936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.1_gamma1</th>\n",
       "      <td>0.702407</td>\n",
       "      <td>0.770363</td>\n",
       "      <td>0.9416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.5_gamma0.01</th>\n",
       "      <td>0.807279</td>\n",
       "      <td>0.886712</td>\n",
       "      <td>0.8648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.5_gamma0.05</th>\n",
       "      <td>0.819334</td>\n",
       "      <td>0.905624</td>\n",
       "      <td>0.8512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.5_gamma0.1</th>\n",
       "      <td>0.811796</td>\n",
       "      <td>0.897975</td>\n",
       "      <td>0.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.5_gamma0.5</th>\n",
       "      <td>0.790716</td>\n",
       "      <td>0.873793</td>\n",
       "      <td>0.8496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0.5_gamma1</th>\n",
       "      <td>0.761116</td>\n",
       "      <td>0.838893</td>\n",
       "      <td>0.8704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1_gamma0.01</th>\n",
       "      <td>0.818322</td>\n",
       "      <td>0.900897</td>\n",
       "      <td>0.8624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1_gamma0.05</th>\n",
       "      <td>0.810789</td>\n",
       "      <td>0.896968</td>\n",
       "      <td>0.8416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1_gamma0.1</th>\n",
       "      <td>0.805766</td>\n",
       "      <td>0.891250</td>\n",
       "      <td>0.8368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1_gamma0.5</th>\n",
       "      <td>0.782704</td>\n",
       "      <td>0.867504</td>\n",
       "      <td>0.8344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1_gamma1</th>\n",
       "      <td>0.753085</td>\n",
       "      <td>0.833826</td>\n",
       "      <td>0.8432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 avg-accuracy  avg-precision  avg-recall\n",
       "c0.01_gamma0.01      0.627198       0.690013      1.0000\n",
       "c0.01_gamma0.05      0.627198       0.690013      1.0000\n",
       "c0.01_gamma0.1       0.654769       0.718544      0.9960\n",
       "c0.01_gamma0.5       0.627198       0.690013      1.0000\n",
       "c0.01_gamma1         0.627198       0.690013      1.0000\n",
       "c0.05_gamma0.01      0.648756       0.712208      0.9976\n",
       "c0.05_gamma0.05      0.789216       0.865033      0.8760\n",
       "c0.05_gamma0.1       0.788206       0.864716      0.8680\n",
       "c0.05_gamma0.5       0.749573       0.821524      0.9104\n",
       "c0.05_gamma1         0.656796       0.722586      0.9776\n",
       "c0.1_gamma0.01       0.766136       0.836136      0.9112\n",
       "c0.1_gamma0.05       0.802256       0.880729      0.8688\n",
       "c0.1_gamma0.1        0.806776       0.885797      0.8696\n",
       "c0.1_gamma0.5        0.776168       0.851007      0.8936\n",
       "c0.1_gamma1          0.702407       0.770363      0.9416\n",
       "c0.5_gamma0.01       0.807279       0.886712      0.8648\n",
       "c0.5_gamma0.05       0.819334       0.905624      0.8512\n",
       "c0.5_gamma0.1        0.811796       0.897975      0.8432\n",
       "c0.5_gamma0.5        0.790716       0.873793      0.8496\n",
       "c0.5_gamma1          0.761116       0.838893      0.8704\n",
       "c1_gamma0.01         0.818322       0.900897      0.8624\n",
       "c1_gamma0.05         0.810789       0.896968      0.8416\n",
       "c1_gamma0.1          0.805766       0.891250      0.8368\n",
       "c1_gamma0.5          0.782704       0.867504      0.8344\n",
       "c1_gamma1            0.753085       0.833826      0.8432"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c_range = [0.01, 0.1, 1.0, 10.0, 100.0]#, 1000.0]            \n",
    "# gamma_range = [0.001, 0.01, 0.1, 1, 10.0, 100] \n",
    "\n",
    "c_range = [0.01, 0.05, 0.1, 0.5, 1] #, 1000.0]            \n",
    "gamma_range = [0.01, 0.05, 0.1, 0.5, 1] \n",
    "\n",
    "DF_PrecisionRecallFscore = pd.DataFrame(np.NaN, index=[\"c\" + str(c) + \"_\" + \"gamma\" + str(gamma) for c, gamma in itertools.product(c_range, gamma_range)],\n",
    "                                        columns=['avg-accuracy', 'avg-precision', 'avg-recall'])\n",
    "\n",
    "outputEvaluationDict, validLabelDict = buildModel(dataContinuous, targetBinomial)\n",
    "\n",
    "for foldNUM, c_gamma_prediction in  outputEvaluationDict.items():\n",
    "#     print ('Running for cross validation fold : ', numFold)\n",
    "    for c_gamma, prediction in c_gamma_prediction.items():\n",
    "        if pd.isnull(DF_PrecisionRecallFscore.ix[c_gamma, 'avg-accuracy']):\n",
    "            DF_PrecisionRecallFscore.ix[c_gamma, 'avg-accuracy'] = metrics.accuracy_score(validLabelDict[foldNUM], prediction)\n",
    "        else:\n",
    "            DF_PrecisionRecallFscore.ix[c_gamma, 'avg-accuracy'] = DF_PrecisionRecallFscore.ix[c_gamma, 'avg-accuracy'] + metrics.accuracy_score(validLabelDict[foldNUM], prediction)\n",
    "            \n",
    "        if pd.isnull(DF_PrecisionRecallFscore.ix[c_gamma, 'avg-precision']):\n",
    "            DF_PrecisionRecallFscore.ix[c_gamma, 'avg-precision'] = metrics.precision_score(validLabelDict[foldNUM], prediction)\n",
    "        else:\n",
    "            DF_PrecisionRecallFscore.ix[c_gamma, 'avg-precision'] = DF_PrecisionRecallFscore.ix[c_gamma, 'avg-accuracy'] + metrics.precision_score(validLabelDict[foldNUM], prediction)\n",
    "            \n",
    "        if pd.isnull(DF_PrecisionRecallFscore.ix[c_gamma, 'avg-recall']):\n",
    "            DF_PrecisionRecallFscore.ix[c_gamma, 'avg-recall'] = metrics.recall_score(validLabelDict[foldNUM], prediction)\n",
    "        else:\n",
    "            DF_PrecisionRecallFscore.ix[c_gamma, 'avg-recall'] = DF_PrecisionRecallFscore.ix[c_gamma, 'avg-recall'] + metrics.recall_score(validLabelDict[foldNUM], prediction)\n",
    "\n",
    "DF_PrecisionRecallFscore/(foldNUM+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Random Forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'accuracy': 0.84199999999999997}, {'precision': 0.86346677944514327}, {'recall': 0.88492710688727683}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifierRF = RandomForestClassifier(n_estimators=500, criterion='gini')  # Entropy and Gini provides very similar evaluation results\n",
    "classifierRF.fit(dataContinuous, targetBinomial)\n",
    "\n",
    "modelName='Random Forest'\n",
    "scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierRF)\n",
    "scoreOBJ.setData(dataIN=dataContinuous, targetIN=targetBinomial)\n",
    "scoreOBJ.cvEvaluate(scoringMetric=['accuracy','precision','recall'])\n",
    "\n",
    "print (modelScores[modelName])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>absFeatureGain</th>\n",
       "      <th>featureGain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>PctKids2Par</td>\n",
       "      <td>0.063366</td>\n",
       "      <td>0.063366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>racePctWhite</td>\n",
       "      <td>0.061412</td>\n",
       "      <td>0.061412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>PctIlleg</td>\n",
       "      <td>0.042549</td>\n",
       "      <td>0.042549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>PctPersDenseHous</td>\n",
       "      <td>0.041143</td>\n",
       "      <td>0.041143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>PctFam2Par</td>\n",
       "      <td>0.038669</td>\n",
       "      <td>0.038669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>FemalePctDiv</td>\n",
       "      <td>0.034174</td>\n",
       "      <td>0.034174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TotalPctDiv</td>\n",
       "      <td>0.031145</td>\n",
       "      <td>0.031145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NumIlleg</td>\n",
       "      <td>0.028525</td>\n",
       "      <td>0.028525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>racepctblack</td>\n",
       "      <td>0.024374</td>\n",
       "      <td>0.024374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>PctYoungKids2Par</td>\n",
       "      <td>0.023780</td>\n",
       "      <td>0.023780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature_name  absFeatureGain  featureGain\n",
       "44       PctKids2Par        0.063366     0.063366\n",
       "3       racePctWhite        0.061412     0.061412\n",
       "50          PctIlleg        0.042549     0.042549\n",
       "68  PctPersDenseHous        0.041143     0.041143\n",
       "43        PctFam2Par        0.038669     0.038669\n",
       "40      FemalePctDiv        0.034174     0.034174\n",
       "41       TotalPctDiv        0.031145     0.031145\n",
       "49          NumIlleg        0.028525     0.028525\n",
       "2       racepctblack        0.024374     0.024374\n",
       "45  PctYoungKids2Par        0.023780     0.023780"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFfeauturePredictivePower = pd.DataFrame(columns=['feature_name','absFeatureGain', 'featureGain'])\n",
    "\n",
    "RFfeauturePredictivePower['feature_name'] = np.array(dataContinuous.columns)\n",
    "RFfeauturePredictivePower['absFeatureGain'] = np.abs(classifierRF.feature_importances_)\n",
    "RFfeauturePredictivePower['featureGain'] = classifierRF.feature_importances_\n",
    "RFfeauturePredictivePower.sort_values('absFeatureGain', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'accuracy': 0.80999999999999994}, {'precision': 0.8353443501939537}, {'recall': 0.85848163563420066}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "classifierAB = AdaBoostClassifier(n_estimators=500, learning_rate=1)  # Entropy and Gini provides very similar evaluation results\n",
    "classifierAB.fit(dataContinuous, targetBinomial)\n",
    "\n",
    "modelName='Ada Boost Classifier'\n",
    "scoreOBJ = ModelScoring(modelName = modelName, modelObj = classifierAB)\n",
    "scoreOBJ.setData(dataIN=dataContinuous, targetIN=targetBinomial)\n",
    "scoreOBJ.cvEvaluate(scoringMetric=['accuracy','precision','recall'])\n",
    "\n",
    "print (modelScores[modelName])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>absFeatureGain</th>\n",
       "      <th>featureGain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>MedOwnCostPctIncNoMtg</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>PctKids2Par</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>PctVacMore6Mos</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>indianPerCap</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>PctTeen2Par</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pctWInvInc</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TotalPctDiv</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>racePctWhite</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>PersPerOccupHous</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>blackPerCap</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature_name  absFeatureGain  featureGain\n",
       "88  MedOwnCostPctIncNoMtg           0.034        0.034\n",
       "44            PctKids2Par           0.024        0.024\n",
       "75         PctVacMore6Mos           0.024        0.024\n",
       "23           indianPerCap           0.024        0.024\n",
       "46            PctTeen2Par           0.022        0.022\n",
       "15             pctWInvInc           0.020        0.020\n",
       "41            TotalPctDiv           0.020        0.020\n",
       "3            racePctWhite           0.020        0.020\n",
       "64       PersPerOccupHous           0.020        0.020\n",
       "22            blackPerCap           0.020        0.020"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABfeauturePredictivePower = pd.DataFrame(columns=['feature_name','absFeatureGain', 'featureGain'])\n",
    "\n",
    "ABfeauturePredictivePower['feature_name'] = np.array(dataContinuous.columns)\n",
    "ABfeauturePredictivePower['absFeatureGain'] = np.abs(classifierAB.feature_importances_)\n",
    "ABfeauturePredictivePower['featureGain'] = classifierAB.feature_importances_\n",
    "ABfeauturePredictivePower.sort_values('absFeatureGain', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0        0.01           0.61          0.21          0.83          0.02   \n",
       "1        0.01           0.41          0.55          0.57          0.01   \n",
       "2        0.03           0.34          0.86          0.30          0.04   \n",
       "3        0.01           0.38          0.35          0.71          0.04   \n",
       "4        0.04           0.37          0.32          0.70          0.21   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up ...    46  \\\n",
       "0         0.01         0.41         0.49         0.26        0.21 ...   0.0   \n",
       "1         0.00         0.47         0.45         0.31        0.57 ...   0.0   \n",
       "2         0.01         0.41         0.42         0.27        0.59 ...   0.0   \n",
       "3         0.01         0.39         0.46         0.31        0.49 ...   0.0   \n",
       "4         0.02         1.00         1.00         1.00        0.14 ...   0.0   \n",
       "\n",
       "    47   48   49   50   51   53   54   55   56  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 146 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding of the factor column state\n",
    "dataMixed = pd.concat([dataContinuous, pd.get_dummies(crimeDF['state'])], axis=1)\n",
    "dataMixed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Call the SVM rbf model with the state column:\n",
    "outputEvaluationDict, validLabelDict = buildModel(dataMixed, targetBinomial)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
